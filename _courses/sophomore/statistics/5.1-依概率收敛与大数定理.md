---
title: 依概率收敛与大数定理
permalink: /courses/sophomore/statistics/依概率收敛与大数定理
layout: article
nav_key: courses
sidebar:
  nav: statistics
license: false
aside:
  toc: true
lang: zh
show_edit_on_github: false
show_date: false
mathjax: true
mathjax_autoNumber: false
mermaid: false
chart: false
---
<!--more-->

$$
\begin{align*}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\p}{\partial}
\end{align*}
$$

# 依概率收敛

之前说过，*“频率的稳定值记为概率”*，其中的 *“稳定”* 并不是指频率的极限 $\lim_{n\rightarrow \infty} \frac{n_A}{n}=p$，而是指 $n$ 充分大时，对于 $\forall \varepsilon>0$，$\vert \frac{n_A}{n}-p \vert \geq \varepsilon$ 的 **可能性** 很小，即：

$$
\lim_{n\rightarrow \infty} P\left\{ \left\vert \frac{n_A}{n}-p \right\vert \geq \varepsilon \right\}=0
$$

也就是说，频率“有可能”不会收敛到概率，但这种可能性很小，因而表现出频率一定会收敛到概率。这种收敛性就称为 **依概率收敛**。详细定义如下：

设 $Y_1,Y_2,\cdots,Y_n,\cdots$ 为随机变量序列，$c$ 为一常数，若对于 $\forall \varepsilon > 0$，均有：

$$
\lim_{n\rightarrow +\infty} P\left\{ \left\vert Y_n -c \right\vert \geq \varepsilon \right\}=0
$$

则称随机变量序列 $\{Y_n\}$ 依概率收敛于 $c$，记为：$Y_n\xrightarrow{P} c,\; n\rightarrow +\infty$

<p class="success">
例题：设 $X_n \sim \text{N}(0,\frac{1}{n})$，证明 $X_n \xrightarrow{P}0$，$n\rightarrow+\infty$
</p>
<p class="info">
解：对于任意 $\varepsilon>0$，<br>
$$
P(\vert X_n- 0\vert \geq \varepsilon)=P(X_n \geq \varepsilon)+P(X_n\leq -\varepsilon)\\
=1-\Phi(\frac{\varepsilon-0}{\sqrt{1/n}})+\Phi(\frac{-\varepsilon-0}{\sqrt{1/n}})\\
=2[1-\Phi(\varepsilon\sqrt{n})]\rightarrow0,\;n\rightarrow\infty
$$
</p>

`性质1`{:.success} 若 $X_n \xrightarrow{P} a$，$Y_n \xrightarrow{P}b$，当 $n\rightarrow\infty$ 时，函数 $g(x,y)$ 在点 $(a,b)$ 处连续，那么：

$$
g(X_n,Y_n) \xrightarrow{P} g(a,b),\;n\rightarrow\infty
$$

由这条性质，可以推导出两个序列之间的运算的依概率收敛：$X_n+Y_n \xrightarrow{P} a+b$

# 切比雪夫不等式

`定理`{:.success}

设随机变量 $X$ 具有数学期望 $E(X)=\mu$，方差 $D(X)=\sigma^2$，则对于任意 $\varepsilon>0$ 都有：

$$
P\{ \vert X-\mu\vert\geq \varepsilon \} \leq \frac{\sigma^2}{\varepsilon^2}
$$

定理的等价形式为：

$$
P\{ \vert X-\mu\vert< \varepsilon \} \geq 1-\frac{\sigma^2}{\varepsilon^2}
$$

`证明`{:.info}

对于任意 $\varepsilon>0$，令：

$$
Z=
\begin{cases}
\varepsilon & |X-\mu|\geq\varepsilon\\
0 & |X-\mu|<\varepsilon
\end{cases}
$$

则 $Z\leq |X-\mu|$，那么 $X$ 的方差 $D(X)=E[(X-\mu)^2]$ 存在时，$E(Z^2)$ 也存在，且 $E(Z^2)\leq D(X)$，根据 $Z$ 的定义：$E(Z^2)=\varepsilon^2 P\{ |X-\mu|\geq \varepsilon \}$，故有：

$$
E(Z^2)=\varepsilon^2 P\{ |X-\mu|\geq \varepsilon \} \leq D(X)\\
即\;P\{ |X-\mu|\geq \varepsilon \} \leq \frac{D(X)}{\varepsilon^2}\;成立
$$

`说明`{:.info}

切比雪夫不等式适用于期望、方差存在的随机变量。它可以对随机变量落在期望附近的区域内或外的概率 给出一个界的估计。比如说标准正态分布落在 $3\sigma$ 内的概率为：$P\{ -3 < X \leq 3 \} = 2 \Phi(3)-1 \approx 0.9974$ $\geq 1-\frac{1}{3^2}=8/9 \approx 0.889$

# 贝努里大数定理

`定理`{:.success} 记 $n_A$ 为 n 重贝努里试验中事件 $A$ 发生的概率，并且每次试验中 $A$ 发生的概率为 $p$，则对于 $\forall \varepsilon >0$，有：

$$
\lim_{n\rightarrow+\infty} P\{ |\frac{n_A}{n}-p|\geq \varepsilon \}=0\\
即\; \frac{n_A}{n} \xrightarrow{P} P,\; n\rightarrow+\infty
$$

`证明`{:.info}

$$
n_A \sim \text{B}(n,p)\\
E(n_A)=np\quad D(n_A)=np(1-p)\\
由切比雪夫不等式，对于 \forall \varepsilon > 0，有：\quad\quad\quad\\
P\{ |\frac{n_A}{n}-p|\geq \varepsilon \}= P\{ |n_A-np|\geq n\varepsilon \}\\
\leq \frac{np(1-p)}{n^2\varepsilon^2}=\frac{p(1-p)}{n\varepsilon^2}\rightarrow 0，
当 n\rightarrow+\infty\\
\therefore \lim_{n\rightarrow+\infty} P\{ |\frac{n_A}{n}-p|\geq \varepsilon \}=0
$$

贝努里大数定理的意义：
* 证明了：大量重复独立试验中，事件出现的频率的极限值可以确定概率
* 提供了通过试验来确定事件概率的方法（将频率作为概率）

# 大数定理

设 $X_1,X_2,\cdots,X_n,\cdots$ 是一列随机变量，则在一定条件下，当 $n\rightarrow\infty$ 随机变量序列 $Y_n=\frac{1}{n}(X_1+\cdots+X_n)$ 收敛到 $\mu$

解释如下：
* “收敛”：指依概率收敛
* “$\mu$”：若 $X_i$ 期望值相同，则 $\mu=E(X_i)$
* “一定条件”：不同的大数定理有不同的条件

`切比雪夫大数定理的推论`{:.success} $X_1,X_2,\cdots,X_n,\cdots$ 为相互独立的随机变量，且具有相同的期望 $\mu$ 和相同的方差 $\sigma^2$，那么：

$$
\frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{P} \mu,\;n\rightarrow+\infty
$$

`证明`{:.info}

记 $Y_n=\frac{1}{n} \sum_{i=1}^n X_i $，则 $E(Y_n)=\mu$，$D(Y_n)=\frac{\sigma^2}{n}$，由切比雪夫不等式，得到：

$$
0\leq P\{|Y_n-E(Y_n)| \geq \varepsilon\} \leq \frac{D(Y_n)}{\varepsilon^2}=\frac{\sigma^2}{n\varepsilon}\rightarrow 0
$$

`注意`{:.warning}

前提条件是：方差存在（因为方差存在可以推导出期望存在，反之不行）



`辛钦大数定律`{:.success} $X_1,X_2,\cdots,X_n,\cdots$ 为相互独立的同分布的随机变量，且其期望存在，记为 $\mu$，那么当 $n\rightarrow+\infty$ 时，$\frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{P} \mu$

辛钦大数定律说明当 $n$ 充分大时，可将 $n$ 次平均看作 $E(X)$ 的近似。



`切比雪夫(弱)大数定理`{:.success} $X_1,X_2,\cdots,X_n,\cdots$ 为相互独立的随机变量，并且方差有同一上界 $D(X_i)\leq C$（$C$ 为大于零的常数），那么当 $n\rightarrow+\infty$ 时，$\frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{P} \mu$，$\mu=E(\frac{1}{n} \sum_{i=1}^n X_i) = \frac{1}{n} \sum_{i=1}^n EX_i$

`证明`{:.info}

$$
D(\frac{X_1+X_2+\cdots+X_n}{n})=\frac{\sum_{i=1}^n D(X_i)}{n^2}\leq \frac{c}{n}\\
由切比雪夫不等式：\\
0 \leq P \left( \left| \frac{X_1+X_2+\cdots+X_n}{n}-\mu \right| \geq \varepsilon \right) \leq \frac{c}{n\varepsilon^2} \rightarrow 0
$$

---

几个大数定理的条件对比：

* 切比雪夫（推论）：不同分布，但具有相同均值和方差
* 辛钦：同分布，且存在均值
* 切比雪夫：不同分布，但方差具有相同上界

---

<p class="success">
例题：设随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 独立同分布，$X_1\sim \text{U}(0,1)$，则 $\sqrt[n]{X_1X_2\cdots X_n}$ 是否依概率收敛？若是，收敛于什么？
</p>
<p class="info">
解：记 $Y_n=\sqrt[n]{X_1X_2\cdots X_n}$，令 $Z_n=\ln Y_n=\frac{1}{n}(\ln X_1+ \cdots + \ln X_n)$<br>
则 $\ln X_1,\cdots, \ln X_n,\cdots$ 独立同分布，并且：<br>
$$
E(\ln X_1)=\int_0^1 \ln x \dif x =-1
$$<br>
那么由辛钦大数定理，知 $Z_n \xrightarrow{P} -1$，$n\rightarrow +\infty$
</p>