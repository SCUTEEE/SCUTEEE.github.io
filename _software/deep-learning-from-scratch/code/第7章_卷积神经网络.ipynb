{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "第7章 卷积神经网络",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wITu8puDDFyx"
      },
      "source": [
        "# 本节需要用的的函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwivpNS4Elk7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))    \n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "    \n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "    \n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 溢出对策\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGM6HrgnDsmF"
      },
      "source": [
        "def _numerical_gradient_1d(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val # 还原值\n",
        "        \n",
        "    return grad\n",
        "\n",
        "\n",
        "def numerical_gradient_2d(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return _numerical_gradient_1d(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "        \n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_1d(f, x)\n",
        "        \n",
        "        return grad\n",
        "\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val # 还原值\n",
        "        it.iternext()   \n",
        "        \n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efhR-L82Edab"
      },
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W =W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 权重和偏置参数的导数\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 对应张量\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None # softmax的输出\n",
        "        self.t = None # 监督数据\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # Conv层的情况下为4维，全连接层的情况下为2维  \n",
        "\n",
        "        # 测试时使用的平均值和方差\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var  \n",
        "        \n",
        "        # backward时使用的中间数据\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "        \n",
        "        return out.reshape(*self.input_shape)\n",
        "            \n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "                        \n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "            \n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "            \n",
        "        out = self.gamma * xn + self.beta \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "        \n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYf6xqnQDruS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "56e67c2a-fee0-410c-988e-2edf8f926b5c"
      },
      "source": [
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('You should use Python 3.x')\n",
        "import os.path\n",
        "from IPython.terminal.embed import InteractiveShellEmbed\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "## if you run in terminal, run this\n",
        "# dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "## if you run in IPython, run this\n",
        "ip_shell = InteractiveShellEmbed()\n",
        "dataset_dir = ip_shell.magic(\"%pwd\")\n",
        "\n",
        "save_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "train_num = 60000\n",
        "test_num = 10000\n",
        "img_dim = (1, 28, 28)\n",
        "img_size = 784\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print(\"Downloading \" + file_name + \" ... \")\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    print(\"Done\")\n",
        "    \n",
        "def download_mnist():\n",
        "    for v in key_file.values():\n",
        "       _download(v)\n",
        "        \n",
        "def _load_label(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    data = data.reshape(-1, img_size)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return data\n",
        "    \n",
        "def _convert_numpy():\n",
        "    dataset = {}\n",
        "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
        "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def init_mnist():\n",
        "    download_mnist()\n",
        "    dataset = _convert_numpy()\n",
        "    print(\"Creating pickle file ...\")\n",
        "    with open(save_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f, -1)\n",
        "    print(\"Done!\")\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "        \n",
        "    return T\n",
        "    \n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    \"\"\"读入MNIST数据集\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    normalize : 将图像的像素值正规化为0.0~1.0\n",
        "    one_hot_label : \n",
        "        one_hot_label为True的情况下，标签作为one-hot数组返回\n",
        "        one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组\n",
        "    flatten : 是否将图像展开为一维数组\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (训练图像, 训练标签), (测试图像, 测试标签)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(save_file):\n",
        "        init_mnist()\n",
        "        \n",
        "    with open(save_file, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    \n",
        "    if normalize:\n",
        "        for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].astype(np.float32)\n",
        "            dataset[key] /= 255.0\n",
        "            \n",
        "    if one_hot_label:\n",
        "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "    \n",
        "    if not flatten:\n",
        "         for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    init_mnist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePT5C9YDDx9g"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class SGD:\n",
        "\n",
        "    \"\"\"随机梯度下降法（Stochastic Gradient Descent）\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key] \n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "    \"\"\"Momentum SGD\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():                                \n",
        "                self.v[key] = np.zeros_like(val)\n",
        "                \n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
        "            params[key] += self.v[key]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
        "        \n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "            \n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
        "            \n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vftUX3XoDP7p"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"进行神经网络的训练的类\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "        \n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "        \n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "\n",
        "    def train_step(self):\n",
        "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "        \n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "        \n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "        if self.verbose: print(\"train loss:\" + str(loss))\n",
        "        \n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "            \n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "                \n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk3WITwIbMMs"
      },
      "source": [
        "# 卷积神经网络\n",
        "\n",
        "**卷积神经网络（Convolutional Neural Newtwork，CNN）** 与之前介绍的神经网络的不同之处在于，之前的神经网络的所有神经元之间都有连接，称为 **全连接（fully-connected）**，这是通过 Affine层（全连接层）实现的。比如下面的一个神经网络：\n",
        "\n",
        "![](https://pic3.zhimg.com/v2-8800531c73144638e59cb134ea7194be_r.jpg)\n",
        "\n",
        "卷积网络中，增加了 Convolution层（卷积层） 和 Pooling层（池化层），将之前的 Affine-ReLU 改成了 Convolution-ReLU-Pooling\n",
        "\n",
        "![](https://pic1.zhimg.com/80/v2-5c608696331dc2dae1f70c28130e9b0b_720w.jpg)\n",
        "\n",
        "下面来介绍新增的这两个层。\n",
        "\n",
        "# 卷积层\n",
        "\n",
        "全连接层有个很大的问题，就是无论数据是几维的，都会拉平成一维数据。比如之前的MNIST数据集，本来是 $28\\times 28$ 的图像，输入时却成了 $784\\times 1$ 的数组。因此，图像中的很多空间信息都损失了。而卷积层则可以接受图像输入，并输出图像，我们将输入输出的数据称为 **特征图（feature map）**。\n",
        "\n",
        "卷积层主要做的是卷积运算，运算过程如下：用一个 **滤波器** 对图像进行扫描，将图像的被扫部分与滤波器进行元素相乘再求和，从而得到结果中的一个子像素。\n",
        "\n",
        "$$\n",
        "\\newcommand{\\blue}[1]{{\\color{Cyan}#1}}\n",
        "\\begin{bmatrix}\n",
        "1 & 2 & 3 & 0\\\\\n",
        "0 & 1 & 2 & 3\\\\\n",
        "3 & 0 & 1 & 2\\\\\n",
        "2 & 3 & 0 & 1\n",
        "\\end{bmatrix} * \n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 1\\\\\n",
        "0 & 1 & 2\\\\\n",
        "1 & 0 & 2\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "15 & 16\\\\\n",
        "6 & 15\n",
        "\\end{bmatrix}\n",
        "\\\\\n",
        "\\begin{bmatrix}\n",
        "15 &  \\\\\n",
        " &  \n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "\\blue{1} & \\blue2 & \\blue3 & 0\\\\\n",
        "\\blue0 & \\blue1 & \\blue2 & 3\\\\\n",
        "\\blue3 & \\blue0 & \\blue1 & 2\\\\\n",
        "2 & 3 & 0 & 1\n",
        "\\end{bmatrix} \\odot\n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 1\\\\\n",
        "0 & 1 & 2\\\\\n",
        "1 & 0 & 2\n",
        "\\end{bmatrix}\n",
        "\\\\\n",
        "\\begin{bmatrix}\n",
        " & 16 \\\\\n",
        " &  \n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "1 & \\blue2 & \\blue3 & \\blue0\\\\\n",
        "0 & \\blue1 & \\blue2 & \\blue3\\\\\n",
        "3 & \\blue0 & \\blue1 & \\blue2\\\\\n",
        "2 & 3 & 0 & 1\n",
        "\\end{bmatrix} \\odot\n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 1\\\\\n",
        "0 & 1 & 2\\\\\n",
        "1 & 0 & 2\n",
        "\\end{bmatrix}\n",
        "\\\\\n",
        "\\begin{bmatrix}\n",
        " &  \\\\\n",
        "6 &  \n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "1 & 2 & 3 & 0\\\\\n",
        "\\blue0 & \\blue1 & \\blue2 & 3\\\\\n",
        "\\blue3 & \\blue0 & \\blue1 & 2\\\\\n",
        "\\blue2 & \\blue3 & \\blue0 & 1\n",
        "\\end{bmatrix} \\odot\n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 1\\\\\n",
        "0 & 1 & 2\\\\\n",
        "1 & 0 & 2\n",
        "\\end{bmatrix}\n",
        "\\\\\n",
        "\\begin{bmatrix}\n",
        " &  \\\\\n",
        " & 15 \n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "1 & 2 & 3 & 0\\\\\n",
        "0 & \\blue1 & \\blue2 & \\blue3\\\\\n",
        "3 & \\blue0 & \\blue1 & \\blue2\\\\\n",
        "2 & \\blue3 & \\blue0 & \\blue1\n",
        "\\end{bmatrix} \\odot\n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 1\\\\\n",
        "0 & 1 & 2\\\\\n",
        "1 & 0 & 2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "卷积层中也有偏置，偏置一般就是一个数字，加到卷积结果的所有元素上。这里就不附图了。\n",
        "\n",
        "注意到上面 $4\\times 4$ 矩阵经过卷积后变成了 $2\\times 2$ 矩阵，说明卷积会缩小图像。如果经过多次卷积，那么图像最终会缩成 $1\\times 1$。为了使得图像大小不改变，我们会对输入数据进行 **填充（padding）**，就是在边上加一圈 0，这样卷积完还是一样大：\n",
        "\n",
        "$$\n",
        "\\newcommand{\\grey}[1]{{\\color{Grey}#1}}\n",
        "\\begin{bmatrix}\n",
        "\\grey0 & \\grey0 & \\grey0 & \\grey0 & \\grey0 & \\grey0\\\\\n",
        "\\grey0 & 1 & 2 & 3 & 0 & \\grey0\\\\\n",
        "\\grey0 & 0 & 1 & 2 & 3 & \\grey0\\\\\n",
        "\\grey0 & 3 & 0 & 1 & 2 & \\grey0\\\\\n",
        "\\grey0 & 2 & 3 & 0 & 1 & \\grey0\\\\\n",
        "\\grey0 & \\grey0 & \\grey0 & \\grey0 & \\grey0 & \\grey0\n",
        "\\end{bmatrix} * \n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 1\\\\\n",
        "0 & 1 & 2\\\\\n",
        "1 & 0 & 2\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "7 & 12 & 10 & 2\\\\\n",
        "4 & 15 & 16 & 10\\\\\n",
        "10 & 6 & 15 & 6\\\\\n",
        "8 & 10 & 4 & 3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "此外，还有一个重要参数就是 **步幅（stride）**，即每次滤波器上下左右移动的距离。前面的步幅是 1，我们也可以改成 3：\n",
        "\n",
        "$$\n",
        "\\newcommand{\\grey}[1]{{\\color{Grey}#1}}\n",
        "\\begin{bmatrix}\n",
        "\\grey0 & \\grey0 & \\grey0 & \\grey0 & \\grey0 & \\grey0\\\\\n",
        "\\grey0 & 1 & 2 & 3 & 0 & \\grey0\\\\\n",
        "\\grey0 & 0 & 1 & 2 & 3 & \\grey0\\\\\n",
        "\\grey0 & 3 & 0 & 1 & 2 & \\grey0\\\\\n",
        "\\grey0 & 2 & 3 & 0 & 1 & \\grey0\\\\\n",
        "\\grey0 & \\grey0 & \\grey0 & \\grey0 & \\grey0 & \\grey0\n",
        "\\end{bmatrix} * \n",
        "\\begin{bmatrix}\n",
        "2 & 0 & 1\\\\\n",
        "0 & 1 & 2\\\\\n",
        "1 & 0 & 2\n",
        "\\end{bmatrix}\n",
        "=\\begin{bmatrix}\n",
        "7 & 2\\\\\n",
        "8 & 3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "假设输入大小为 $(H,W)$，滤波器大小为 $(FH,FW)$，填充为 $P$，步幅为 $S$，那么输出大小 $(OH,OW)$ 满足：\n",
        "\n",
        "$$\n",
        "OH = \\frac{H+2P-FH}{S}+1\\\\\n",
        "OW = \\frac{W+2P-FW}{S}+1\n",
        "$$\n",
        "\n",
        "如果除不尽的话，可以报错，也可以四舍五入。\n",
        "\n",
        "对于高维数据，可以用高维滤波器进行卷积。下图展示了一个三维数据经过三维滤波器卷积后，得到了一个一维数据。\n",
        "\n",
        "![](https://pic4.zhimg.com/80/v2-54e419521023caf0090aecf012a12d70_720w.jpg)\n",
        "\n",
        "或者也可以用多个的滤波器进行卷积，这样会得到一个多维的数据：\n",
        "\n",
        "![](https://pic4.zhimg.com/80/v2-10da37e0c4cf860155ac16b817b58429_720w.jpg)\n",
        "\n",
        "对于有多个通道的图像（3维数据），我们一般按照（channel, height, width）的顺序写，滤波器也同理。我们可以用一个方块的厚度、长、宽来表示，如下图（或上图）：\n",
        "\n",
        "![](https://i.loli.net/2020/09/13/PkA9gm1vGweUyWz.png)\n",
        "\n",
        "卷积同样可以进行批处理，此时一个batch的数据写成（batch_size, channel, height, width）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpDV4aAoyszP"
      },
      "source": [
        "# 池化层\n",
        "\n",
        "池化层与卷积层类似，但它并不是相乘再求和，而是取最大值（Max池化）或平均值（Average池化）。比如：\n",
        "\n",
        "$$\n",
        "\\newcommand{\\blue}[1]{{\\color{Cyan}#1}}\n",
        "\\newcommand{\\red}[1]{{\\color{Magenta}#1}}\n",
        "\\newcommand{\\orange}[1]{{\\color{Orange}#1}}\n",
        "\\newcommand{\\green}[1]{{\\color{Lime}#1}}\n",
        "\\begin{bmatrix}\n",
        "\\blue1 & \\blue2 & \\red3 & \\red0\\\\\n",
        "\\blue0 & \\blue1 & \\red2 & \\red3\\\\\n",
        "\\orange3 & \\orange0 & \\green1 & \\green2\\\\\n",
        "\\orange2 & \\orange3 & \\green0 & \\green1\n",
        "\\end{bmatrix} \n",
        "\\Rightarrow\n",
        "\\begin{bmatrix}\n",
        "\\blue2 & \\red3\\\\\n",
        "\\orange4 & \\green5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "一般来说，池化的窗口大小与步幅会设定成相同的值。池化层与卷积的不同之处在于：\n",
        "\n",
        "* 池化层没有要学习的参数，只是取最大或平均值\n",
        "* 数据经过池化层后通道数不发生变换，各通道是独立进行的\n",
        "* 池化层对微小的变化具有鲁棒性（这样图像就算有微小的位移也不会造成影响）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP04Oreo1Tb6"
      },
      "source": [
        "# 卷积层的实现\n",
        "\n",
        "如果使用循环来实现卷积，那么就会十分复杂且速度很慢。这里我们利用 im2col 来实现。im2col 会将图像中每个滤波区域横向转化成一列（我也不知道为什么不说“转化为一行”），将滤波器纵向转化成一列，这样，就可以通过矩阵相乘一次性得到卷积结果，然后再 reshape 为输出数据的大小。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWiC5i8E25-F"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
        "    return (input_size + 2*pad - filter_size) / stride + 1\n",
        "\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据\n",
        "    filter_h : 滤波器的高\n",
        "    filter_w : 滤波器的长\n",
        "    stride : 步幅\n",
        "    pad : 填充\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2维数组\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape #获取数据的形状\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1 #输出数据的高度\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1 #输出数据的宽度\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant') #填充\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col :\n",
        "    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）\n",
        "    filter_h :\n",
        "    filter_w\n",
        "    stride\n",
        "    pad\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkBhzhXq5x9o"
      },
      "source": [
        "下面我们来验证一下 im2col 的正确性。这里有一个 $3$ 通道的 $7\\times 7$ 数据，要被 $3\\times 5 \\times 5$ 滤波，得到 $3\\times3$ 的输出。也就是说，要做 $9$ 次卷积，每次卷积包含 $3\\times5\\times5=75$ 次乘法。所以 im2col 后应该得到一个 $(9,75)$ 的矩阵。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJoORhdY5LEX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e30969f2-1ff9-4921-ddc3-56d2332dc4b9"
      },
      "source": [
        "x = np.random.rand(1,3,7,7)\n",
        "col1 = im2col(x,5,5,stride=1,pad=0)\n",
        "print(col1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9, 75)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeXXMZn67uhe"
      },
      "source": [
        "经过 im2col 后，卷积和Affine十分相似，其反向传播的过程也是类似的。实现方法如下："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPWq7K4B7Urf"
      },
      "source": [
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 中间数据（backward时使用）\n",
        "        self.x = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 权重和偏置参数的梯度\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWVvb-GY8fLe"
      },
      "source": [
        "# 池化层的实现\n",
        "\n",
        "池化层也是利用 im2col 来实现。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M52Zrwzm89bW"
      },
      "source": [
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkFMs57yA8vl"
      },
      "source": [
        "# 构建CNN\n",
        "\n",
        "下面我们来构建一个简单的CNN网络用于识别 MNIST，其构成为：\n",
        "\n",
        "* Conv\n",
        "* ReLU\n",
        "* Pooling\n",
        "* Affine\n",
        "* ReLU\n",
        "* Affine\n",
        "* Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCRqSZt9BdWW"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \"\"\"简单的ConvNet\n",
        "\n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 输入大小（MNIST的情况下为784）\n",
        "    hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）\n",
        "    output_size : 输出大小（MNIST的情况下为10）\n",
        "    activation : 'relu' or 'sigmoid'\n",
        "    weight_init_std : 指定权重的标准差（e.g. 0.01）\n",
        "        指定'relu'或'he'的情况下设定“He的初始值”\n",
        "        指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 初始化权重\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 生成层\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"求损失函数\n",
        "        参数x是输入数据、t是教师标签\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"求梯度（数值微分）\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 输入数据\n",
        "        t : 教师标签\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        具有各层的梯度的字典变量\n",
        "            grads['W1']、grads['W2']、...是各层的权重\n",
        "            grads['b1']、grads['b2']、...是各层的偏置\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"求梯度（误差反向传播法）\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 输入数据\n",
        "        t : 教师标签\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        具有各层的梯度的字典变量\n",
        "            grads['W1']、grads['W2']、...是各层的权重\n",
        "            grads['b1']、grads['b2']、...是各层的偏置\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 设定\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pDt0ns8C-5C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6fd07e3-d56e-47d4-8e35-6986e167203c"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 读入数据\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 处理花费时间较长的情况下减少数据 \n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 保存参数\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 绘制图形\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.300146814668723\n",
            "=== epoch:1, train acc:0.199, test acc:0.22 ===\n",
            "train loss:2.2985250734577347\n",
            "train loss:2.296146113765854\n",
            "train loss:2.2935921167306375\n",
            "train loss:2.284482711212491\n",
            "train loss:2.269761730908964\n",
            "train loss:2.26717613914087\n",
            "train loss:2.2497892677789117\n",
            "train loss:2.2254723518288233\n",
            "train loss:2.200770598081398\n",
            "train loss:2.178102425453755\n",
            "train loss:2.151542371911034\n",
            "train loss:2.115589067821732\n",
            "train loss:2.0703534921832767\n",
            "train loss:2.046689168848587\n",
            "train loss:1.9776030735462873\n",
            "train loss:1.9422454178979023\n",
            "train loss:1.9277278577021715\n",
            "train loss:1.8024423627939878\n",
            "train loss:1.8031761099333685\n",
            "train loss:1.6973572093494917\n",
            "train loss:1.4855934144353369\n",
            "train loss:1.561868158774375\n",
            "train loss:1.4932367589160194\n",
            "train loss:1.4268131551573575\n",
            "train loss:1.2781116112804225\n",
            "train loss:1.1626730307403186\n",
            "train loss:1.1760282684415702\n",
            "train loss:1.0780982964519876\n",
            "train loss:1.0945119981045135\n",
            "train loss:0.9776328033889985\n",
            "train loss:0.8831707224191346\n",
            "train loss:0.887287952041547\n",
            "train loss:1.0472884297811793\n",
            "train loss:0.8450906151694685\n",
            "train loss:0.8147775749298017\n",
            "train loss:0.6931920689494276\n",
            "train loss:0.8388325326751407\n",
            "train loss:0.8860606661389099\n",
            "train loss:0.7243942912551817\n",
            "train loss:0.5898460748148313\n",
            "train loss:0.8575892076414143\n",
            "train loss:0.5367257439430706\n",
            "train loss:0.6195133852103063\n",
            "train loss:0.9130676889137189\n",
            "train loss:0.548970953382532\n",
            "train loss:0.7624903884113943\n",
            "train loss:0.5622640561040725\n",
            "train loss:0.5714598056774205\n",
            "train loss:0.7303078399591212\n",
            "train loss:0.500208872534865\n",
            "train loss:0.6688124926944106\n",
            "train loss:0.8141564696288763\n",
            "train loss:0.4638347825339652\n",
            "train loss:0.5276622692725408\n",
            "train loss:0.67907310627936\n",
            "train loss:0.610283856634882\n",
            "train loss:0.4701378849317194\n",
            "train loss:0.4894873987947294\n",
            "train loss:0.6090240875446304\n",
            "train loss:0.44412260432255807\n",
            "train loss:0.49427816695883997\n",
            "train loss:0.4788915973130125\n",
            "train loss:0.42892317698496407\n",
            "train loss:0.5366379074723624\n",
            "train loss:0.33140448672074185\n",
            "train loss:0.48778555677338997\n",
            "train loss:0.3895500647967049\n",
            "train loss:0.5228409170670962\n",
            "train loss:0.5062947293429119\n",
            "train loss:0.4143644109587716\n",
            "train loss:0.5248182052702554\n",
            "train loss:0.43834892744531145\n",
            "train loss:0.37357880598718063\n",
            "train loss:0.4803411144422013\n",
            "train loss:0.36753925533380005\n",
            "train loss:0.42219436302362107\n",
            "train loss:0.35263318217766143\n",
            "train loss:0.43787169123871406\n",
            "train loss:0.42315063902607086\n",
            "train loss:0.4179825663617311\n",
            "train loss:0.490441673260354\n",
            "train loss:0.3276221996752644\n",
            "train loss:0.41889883596099275\n",
            "train loss:0.5510975579566135\n",
            "train loss:0.37946859691417556\n",
            "train loss:0.4600570111360646\n",
            "train loss:0.5128801147468175\n",
            "train loss:0.6225506149151484\n",
            "train loss:0.31318540294998326\n",
            "train loss:0.31083711734023967\n",
            "train loss:0.533640935600832\n",
            "train loss:0.36329030882646873\n",
            "train loss:0.3733710364063306\n",
            "train loss:0.5629123821368309\n",
            "train loss:0.27680484888649964\n",
            "train loss:0.4026085866455425\n",
            "train loss:0.4015198493575628\n",
            "train loss:0.3266954529808953\n",
            "train loss:0.3370502052480091\n",
            "train loss:0.4002186646186789\n",
            "train loss:0.2983939283758167\n",
            "train loss:0.43963414536608936\n",
            "train loss:0.2729738400399254\n",
            "train loss:0.24402833350794026\n",
            "train loss:0.4862861716515432\n",
            "train loss:0.37478367726875483\n",
            "train loss:0.4058083048581488\n",
            "train loss:0.3190785594265113\n",
            "train loss:0.3526584471320722\n",
            "train loss:0.47851403259615294\n",
            "train loss:0.25299223367937207\n",
            "train loss:0.3981622095452005\n",
            "train loss:0.32267880931448106\n",
            "train loss:0.29506026881401765\n",
            "train loss:0.4555564556315243\n",
            "train loss:0.6167612049327046\n",
            "train loss:0.3759982160910371\n",
            "train loss:0.2487712965309571\n",
            "train loss:0.45570900058174246\n",
            "train loss:0.4176425816944444\n",
            "train loss:0.46394379696944776\n",
            "train loss:0.4371415623452018\n",
            "train loss:0.5109200859642747\n",
            "train loss:0.47966758361616\n",
            "train loss:0.3591911421395649\n",
            "train loss:0.3376842982641206\n",
            "train loss:0.4261684964478655\n",
            "train loss:0.47621623840683214\n",
            "train loss:0.4778904581909261\n",
            "train loss:0.37624846243595994\n",
            "train loss:0.45255718892943136\n",
            "train loss:0.413650967992367\n",
            "train loss:0.25920746054545984\n",
            "train loss:0.46953140810350696\n",
            "train loss:0.31962350108502785\n",
            "train loss:0.2996643419776735\n",
            "train loss:0.3578058942413121\n",
            "train loss:0.349956766132863\n",
            "train loss:0.36909849893250024\n",
            "train loss:0.2823819563120144\n",
            "train loss:0.34517052686704963\n",
            "train loss:0.3431827927140531\n",
            "train loss:0.4228382900013195\n",
            "train loss:0.5208986145531295\n",
            "train loss:0.2652960674663212\n",
            "train loss:0.2509126483950724\n",
            "train loss:0.4799322757984294\n",
            "train loss:0.4782414000126739\n",
            "train loss:0.4405386285667414\n",
            "train loss:0.3744760073543054\n",
            "train loss:0.16075471789972112\n",
            "train loss:0.44950772311862797\n",
            "train loss:0.39994268756495077\n",
            "train loss:0.34515913513628804\n",
            "train loss:0.3830255782593148\n",
            "train loss:0.2984839631391744\n",
            "train loss:0.5566029195035406\n",
            "train loss:0.22472946855262801\n",
            "train loss:0.3497731476564067\n",
            "train loss:0.35372059456477145\n",
            "train loss:0.3031916883718549\n",
            "train loss:0.29247092522264095\n",
            "train loss:0.38770051321687937\n",
            "train loss:0.4019290130706407\n",
            "train loss:0.24007367489533085\n",
            "train loss:0.38216413900702617\n",
            "train loss:0.30743572600258984\n",
            "train loss:0.31708570801114083\n",
            "train loss:0.23363517880260765\n",
            "train loss:0.27940979427399104\n",
            "train loss:0.22545599193579136\n",
            "train loss:0.24196873091700455\n",
            "train loss:0.29422790777680236\n",
            "train loss:0.19356808651290996\n",
            "train loss:0.28145456795705354\n",
            "train loss:0.39099434846541925\n",
            "train loss:0.35698317046792666\n",
            "train loss:0.28785766497160825\n",
            "train loss:0.2957502517249431\n",
            "train loss:0.3031935048531476\n",
            "train loss:0.4392314538987906\n",
            "train loss:0.23103500489136075\n",
            "train loss:0.2091824631243534\n",
            "train loss:0.5302095736590353\n",
            "train loss:0.4085298151320265\n",
            "train loss:0.15224219436212919\n",
            "train loss:0.2854856405216169\n",
            "train loss:0.33627020967149784\n",
            "train loss:0.4260982967409693\n",
            "train loss:0.27200365549556954\n",
            "train loss:0.370228424617255\n",
            "train loss:0.2691679756566093\n",
            "train loss:0.4023340301328308\n",
            "train loss:0.2940073945460174\n",
            "train loss:0.20061699981627712\n",
            "train loss:0.29591342095827167\n",
            "train loss:0.21582560236155715\n",
            "train loss:0.3544997713910001\n",
            "train loss:0.24889073537550538\n",
            "train loss:0.30863045303635794\n",
            "train loss:0.2776991750025759\n",
            "train loss:0.31405175084005976\n",
            "train loss:0.6545070924899945\n",
            "train loss:0.3139071914968918\n",
            "train loss:0.3018491378466408\n",
            "train loss:0.40044931865368383\n",
            "train loss:0.24568813511639795\n",
            "train loss:0.36507269695002315\n",
            "train loss:0.2394551340283109\n",
            "train loss:0.42846622570529613\n",
            "train loss:0.3197450912943097\n",
            "train loss:0.28741697539215677\n",
            "train loss:0.2880400804119433\n",
            "train loss:0.2955382839520427\n",
            "train loss:0.2277091575146126\n",
            "train loss:0.38178297091828417\n",
            "train loss:0.19710063525135527\n",
            "train loss:0.4201661346917821\n",
            "train loss:0.2916543926753712\n",
            "train loss:0.22658478933500945\n",
            "train loss:0.32966380931978007\n",
            "train loss:0.42512556980590466\n",
            "train loss:0.18356231616739335\n",
            "train loss:0.33248237452494367\n",
            "train loss:0.4360207918362501\n",
            "train loss:0.3120829943628792\n",
            "train loss:0.18592383869588874\n",
            "train loss:0.2801110821168104\n",
            "train loss:0.4374590655287936\n",
            "train loss:0.41007411999982823\n",
            "train loss:0.2011360732078317\n",
            "train loss:0.3114944760439029\n",
            "train loss:0.4046443602948947\n",
            "train loss:0.1630554110804342\n",
            "train loss:0.37675034050037676\n",
            "train loss:0.2700805178438424\n",
            "train loss:0.3266013374451367\n",
            "train loss:0.2774965651753593\n",
            "train loss:0.30909211698247424\n",
            "train loss:0.27529510462701934\n",
            "train loss:0.2176887966780274\n",
            "train loss:0.39939743642507186\n",
            "train loss:0.20431113373659673\n",
            "train loss:0.2639359082141461\n",
            "train loss:0.2693991546795603\n",
            "train loss:0.24313264253655387\n",
            "train loss:0.3250105972833833\n",
            "train loss:0.23481443692733542\n",
            "train loss:0.2319394856851242\n",
            "train loss:0.3928920644393863\n",
            "train loss:0.2085173114051279\n",
            "train loss:0.2929818073240066\n",
            "train loss:0.2786766305699511\n",
            "train loss:0.3715437168869725\n",
            "train loss:0.2668643106009495\n",
            "train loss:0.12095896422102893\n",
            "train loss:0.18300284153836455\n",
            "train loss:0.22923829959902509\n",
            "train loss:0.2781730302791275\n",
            "train loss:0.26137535394976785\n",
            "train loss:0.2689884319943794\n",
            "train loss:0.19414329806809494\n",
            "train loss:0.2533590148302907\n",
            "train loss:0.18684041003843443\n",
            "train loss:0.3648586202480681\n",
            "train loss:0.35429277469589776\n",
            "train loss:0.1853735419470425\n",
            "train loss:0.21334619362762994\n",
            "train loss:0.17763970459374057\n",
            "train loss:0.15101522283264826\n",
            "train loss:0.3328689182850642\n",
            "train loss:0.24990719746244713\n",
            "train loss:0.18915632062262336\n",
            "train loss:0.4075608151044946\n",
            "train loss:0.20678614747547375\n",
            "train loss:0.20593070153703097\n",
            "train loss:0.19894776194890273\n",
            "train loss:0.1907406231481383\n",
            "train loss:0.19999843676225448\n",
            "train loss:0.265101495291\n",
            "train loss:0.33134236009501783\n",
            "train loss:0.2557925937991625\n",
            "train loss:0.22315439633642964\n",
            "train loss:0.3307206744142085\n",
            "train loss:0.1275289713167546\n",
            "train loss:0.26577186771879\n",
            "train loss:0.2676056079908953\n",
            "train loss:0.2635431095579449\n",
            "train loss:0.21879288996751106\n",
            "train loss:0.18594365421003492\n",
            "train loss:0.1930365563119697\n",
            "train loss:0.2406218616583653\n",
            "train loss:0.19637065996492187\n",
            "train loss:0.1779119969013437\n",
            "train loss:0.23939706888672888\n",
            "train loss:0.3073779764602056\n",
            "train loss:0.18820810505556992\n",
            "train loss:0.1774953730532655\n",
            "train loss:0.3057766983339211\n",
            "train loss:0.1473504156081677\n",
            "train loss:0.1528514203697411\n",
            "train loss:0.09749718263147876\n",
            "train loss:0.15798688741780129\n",
            "train loss:0.1675382516415425\n",
            "train loss:0.3495686362163324\n",
            "train loss:0.2475715651755365\n",
            "train loss:0.18075426341878453\n",
            "train loss:0.180057470747175\n",
            "train loss:0.2524695789111012\n",
            "train loss:0.24301572748758413\n",
            "train loss:0.2737714770006721\n",
            "train loss:0.3039188689773838\n",
            "train loss:0.16802723824804464\n",
            "train loss:0.2995869140135405\n",
            "train loss:0.11934014041908192\n",
            "train loss:0.23227748538892398\n",
            "train loss:0.2087351714980359\n",
            "train loss:0.42801836921068104\n",
            "train loss:0.17727867574341422\n",
            "train loss:0.20443227913160442\n",
            "train loss:0.14912448593827543\n",
            "train loss:0.1698075836182092\n",
            "train loss:0.2805587803845732\n",
            "train loss:0.23327488094012339\n",
            "train loss:0.19794056494386492\n",
            "train loss:0.34391476072803073\n",
            "train loss:0.29405625526874113\n",
            "train loss:0.306435523152642\n",
            "train loss:0.1657573561554282\n",
            "train loss:0.31889548218237446\n",
            "train loss:0.19886296662233094\n",
            "train loss:0.15138702106474797\n",
            "train loss:0.19852641192111375\n",
            "train loss:0.18016975884550582\n",
            "train loss:0.2895819662303643\n",
            "train loss:0.13761835585837445\n",
            "train loss:0.2510388440456671\n",
            "train loss:0.2201062270749456\n",
            "train loss:0.22574988105493357\n",
            "train loss:0.18327389348575626\n",
            "train loss:0.18479295629256953\n",
            "train loss:0.12330745882047726\n",
            "train loss:0.3715885855104956\n",
            "train loss:0.26057265929143936\n",
            "train loss:0.25726171806893033\n",
            "train loss:0.20858192707475826\n",
            "train loss:0.13375524036110018\n",
            "train loss:0.18873998340253612\n",
            "train loss:0.2951010979586738\n",
            "train loss:0.18736157731463757\n",
            "train loss:0.19490327078315214\n",
            "train loss:0.15172525784983842\n",
            "train loss:0.19567359293622866\n",
            "train loss:0.19577800506280874\n",
            "train loss:0.17679429297185337\n",
            "train loss:0.35451555621888436\n",
            "train loss:0.18948971505804488\n",
            "train loss:0.2393533081505151\n",
            "train loss:0.17362398855556754\n",
            "train loss:0.16975682303553974\n",
            "train loss:0.1941777675268495\n",
            "train loss:0.18855545743193908\n",
            "train loss:0.22042329880530093\n",
            "train loss:0.18791104318010163\n",
            "train loss:0.17697742737082287\n",
            "train loss:0.30758205892513546\n",
            "train loss:0.1563954069362296\n",
            "train loss:0.2341554497123952\n",
            "train loss:0.3399748098083265\n",
            "train loss:0.19319107069110897\n",
            "train loss:0.18617397846077122\n",
            "train loss:0.28674505759705504\n",
            "train loss:0.21090098300243704\n",
            "train loss:0.25751779485930826\n",
            "train loss:0.18527661757890587\n",
            "train loss:0.26590892178203196\n",
            "train loss:0.2885660438800676\n",
            "train loss:0.2628108212904198\n",
            "train loss:0.11091589725451445\n",
            "train loss:0.17567458357954244\n",
            "train loss:0.17122811216409942\n",
            "train loss:0.4222533009510366\n",
            "train loss:0.199994390265969\n",
            "train loss:0.09293387789539695\n",
            "train loss:0.14469298720024865\n",
            "train loss:0.23392758910342917\n",
            "train loss:0.16782351131943557\n",
            "train loss:0.13756084017362857\n",
            "train loss:0.2053630676739053\n",
            "train loss:0.14651467641581697\n",
            "train loss:0.26742959733873245\n",
            "train loss:0.2361498367290472\n",
            "train loss:0.1851155044865551\n",
            "train loss:0.12053494813820227\n",
            "train loss:0.20360409903125493\n",
            "train loss:0.13048513038554735\n",
            "train loss:0.17895799036818705\n",
            "train loss:0.2294827889417114\n",
            "train loss:0.2749598143399418\n",
            "train loss:0.1153994625465363\n",
            "train loss:0.23957362551235614\n",
            "train loss:0.1856685118390969\n",
            "train loss:0.2164216918233817\n",
            "train loss:0.12804529258621009\n",
            "train loss:0.2184123988031191\n",
            "train loss:0.3068987289350476\n",
            "train loss:0.13405977752918571\n",
            "train loss:0.11844603584867613\n",
            "train loss:0.13810819736075858\n",
            "train loss:0.12798107857162733\n",
            "train loss:0.11622978818168449\n",
            "train loss:0.18800509335135995\n",
            "train loss:0.13408218103285904\n",
            "train loss:0.22955618314418466\n",
            "train loss:0.19669198096464593\n",
            "train loss:0.12690777675621537\n",
            "train loss:0.2960899075089929\n",
            "train loss:0.2949079629563055\n",
            "train loss:0.1493962814535447\n",
            "train loss:0.11317155169626446\n",
            "train loss:0.22780820884047556\n",
            "train loss:0.23118984728843905\n",
            "train loss:0.15631703028043048\n",
            "train loss:0.10730228626717199\n",
            "train loss:0.1586688045455067\n",
            "train loss:0.20146397002275618\n",
            "train loss:0.14920978141110874\n",
            "train loss:0.20442508903142234\n",
            "train loss:0.1941477760541363\n",
            "train loss:0.15802588376066543\n",
            "train loss:0.09855428264210758\n",
            "train loss:0.11855284465292674\n",
            "train loss:0.1827248467768329\n",
            "train loss:0.26872402142049534\n",
            "train loss:0.18922803848626493\n",
            "train loss:0.18379157220906311\n",
            "train loss:0.24856869420003339\n",
            "train loss:0.17217535820940694\n",
            "train loss:0.21577355097579706\n",
            "train loss:0.1478800356160002\n",
            "train loss:0.07093775344821085\n",
            "train loss:0.1565721883961624\n",
            "train loss:0.2149071343963954\n",
            "train loss:0.16754489495280128\n",
            "train loss:0.1946342379375438\n",
            "train loss:0.11321388835301315\n",
            "train loss:0.11715961023153082\n",
            "train loss:0.14204819302455143\n",
            "train loss:0.2017638774056294\n",
            "train loss:0.09349054171834517\n",
            "train loss:0.17312209037056273\n",
            "train loss:0.08863058819767511\n",
            "train loss:0.11135111020965902\n",
            "train loss:0.07695534296657222\n",
            "train loss:0.1682653267501958\n",
            "train loss:0.09681327715444488\n",
            "train loss:0.06589060392123827\n",
            "train loss:0.30313307770114983\n",
            "train loss:0.14140359518297896\n",
            "train loss:0.08133205338305294\n",
            "train loss:0.20295036232314514\n",
            "train loss:0.18178519747160954\n",
            "train loss:0.14396473939033755\n",
            "train loss:0.11971835377529612\n",
            "train loss:0.17887959123024066\n",
            "train loss:0.1272206084349089\n",
            "train loss:0.21503656197947196\n",
            "train loss:0.15712023399014702\n",
            "train loss:0.12841869282523655\n",
            "train loss:0.10333011423381126\n",
            "train loss:0.12257307055990921\n",
            "train loss:0.19034700885579703\n",
            "train loss:0.14929980087547468\n",
            "train loss:0.16265144828603767\n",
            "train loss:0.13675424794079624\n",
            "train loss:0.3109986129358358\n",
            "train loss:0.20654711928011038\n",
            "train loss:0.11738901246598984\n",
            "train loss:0.13716004494974313\n",
            "train loss:0.24867234270224348\n",
            "train loss:0.15183116904827823\n",
            "train loss:0.1676106433085534\n",
            "train loss:0.20000546455472232\n",
            "train loss:0.1473232776357835\n",
            "train loss:0.11422684834222252\n",
            "train loss:0.12106152503445061\n",
            "train loss:0.18891396198380103\n",
            "train loss:0.10961114141174162\n",
            "train loss:0.13277706465336675\n",
            "train loss:0.2571977613777474\n",
            "train loss:0.08528657497722912\n",
            "train loss:0.12854359783448827\n",
            "train loss:0.08475531407301017\n",
            "train loss:0.18611403068755028\n",
            "train loss:0.1701095099526439\n",
            "train loss:0.13661554988704275\n",
            "train loss:0.11405655317492046\n",
            "train loss:0.2805521735882128\n",
            "train loss:0.07757210294127206\n",
            "train loss:0.16291031424824548\n",
            "train loss:0.2347497376600447\n",
            "train loss:0.2246992146712338\n",
            "train loss:0.2028909595450863\n",
            "train loss:0.15596294823140747\n",
            "train loss:0.14846144166761124\n",
            "train loss:0.11071511284990632\n",
            "train loss:0.18452711506382052\n",
            "train loss:0.1204401337300353\n",
            "train loss:0.14509286165189747\n",
            "train loss:0.23869992767324313\n",
            "train loss:0.13200727506224708\n",
            "train loss:0.13534384545899342\n",
            "train loss:0.1392467564163238\n",
            "train loss:0.06810857464870784\n",
            "train loss:0.18069459169302024\n",
            "train loss:0.06774256432421134\n",
            "train loss:0.11576177408784159\n",
            "train loss:0.08808936726169575\n",
            "train loss:0.1066991703824679\n",
            "train loss:0.15141046144211703\n",
            "train loss:0.07927819685870373\n",
            "train loss:0.19537291016258818\n",
            "train loss:0.11880690278268839\n",
            "train loss:0.20915339374999525\n",
            "train loss:0.20724398130076152\n",
            "train loss:0.14301581621723852\n",
            "train loss:0.07280320500163757\n",
            "train loss:0.16277432974321873\n",
            "train loss:0.1583678676049186\n",
            "train loss:0.30060597255396737\n",
            "train loss:0.20619683042332604\n",
            "train loss:0.11514467792904969\n",
            "train loss:0.09327105087742411\n",
            "train loss:0.28001528002382514\n",
            "train loss:0.14203786536871785\n",
            "train loss:0.19320966950403362\n",
            "train loss:0.1528470459003609\n",
            "train loss:0.12046251978538247\n",
            "train loss:0.1516997973323325\n",
            "train loss:0.2106679057708487\n",
            "train loss:0.2823138676824443\n",
            "train loss:0.125310238419048\n",
            "train loss:0.11757041759622944\n",
            "train loss:0.12895385043587657\n",
            "train loss:0.22181475915033777\n",
            "train loss:0.13876677016299244\n",
            "train loss:0.1314334191171589\n",
            "train loss:0.18462597978633055\n",
            "train loss:0.17967287771817847\n",
            "train loss:0.0892574943812849\n",
            "train loss:0.15601636540971747\n",
            "train loss:0.08071008315747115\n",
            "train loss:0.13592358743540348\n",
            "train loss:0.1471563494200986\n",
            "train loss:0.09681843824719576\n",
            "train loss:0.08769914848213603\n",
            "train loss:0.17000329189709137\n",
            "train loss:0.1513745186384728\n",
            "train loss:0.09925666707537317\n",
            "train loss:0.10556520512076467\n",
            "train loss:0.11277210858095671\n",
            "train loss:0.12227437313228345\n",
            "train loss:0.2070954804602877\n",
            "train loss:0.07993033008012679\n",
            "train loss:0.12340567382529358\n",
            "train loss:0.1852450991385314\n",
            "train loss:0.1724396741777524\n",
            "train loss:0.25638351327021064\n",
            "train loss:0.14064305022437318\n",
            "train loss:0.12171763397469353\n",
            "train loss:0.16903265402060186\n",
            "train loss:0.18467421198184153\n",
            "train loss:0.06945664493778718\n",
            "train loss:0.08527899536308775\n",
            "train loss:0.09754223657832511\n",
            "train loss:0.049089191755378916\n",
            "train loss:0.15926175976583745\n",
            "train loss:0.20510426178165414\n",
            "train loss:0.05507947997701836\n",
            "train loss:0.1182394374754332\n",
            "train loss:0.17621526046641992\n",
            "train loss:0.20452806285236225\n",
            "train loss:0.08123093440094666\n",
            "train loss:0.1115706755610869\n",
            "train loss:0.14055923489144673\n",
            "train loss:0.1745422877698032\n",
            "train loss:0.09714870120363588\n",
            "train loss:0.08412983676140705\n",
            "train loss:0.08486914712817216\n",
            "train loss:0.07612380696394988\n",
            "train loss:0.1395188646443002\n",
            "train loss:0.15613083877433634\n",
            "train loss:0.09956873837586\n",
            "train loss:0.07962796908338708\n",
            "train loss:0.1954453362725619\n",
            "train loss:0.12720760212301868\n",
            "train loss:0.09261358794847982\n",
            "train loss:0.07691800842826119\n",
            "train loss:0.08331390451554785\n",
            "train loss:0.1914763357012193\n",
            "=== epoch:2, train acc:0.956, test acc:0.955 ===\n",
            "train loss:0.0798549458497342\n",
            "train loss:0.17358857527774257\n",
            "train loss:0.18329885564100235\n",
            "train loss:0.137118468373511\n",
            "train loss:0.12349060791244006\n",
            "train loss:0.23889814775659565\n",
            "train loss:0.14940005471570708\n",
            "train loss:0.07423053738735122\n",
            "train loss:0.09170625695971006\n",
            "train loss:0.07732597276705096\n",
            "train loss:0.11113493089939917\n",
            "train loss:0.29235450429299237\n",
            "train loss:0.10132665172662211\n",
            "train loss:0.1905820252098211\n",
            "train loss:0.028928465853182386\n",
            "train loss:0.20556897312605982\n",
            "train loss:0.16038326124898789\n",
            "train loss:0.0966609790560968\n",
            "train loss:0.14104177757254482\n",
            "train loss:0.15293790385383724\n",
            "train loss:0.23213316017760174\n",
            "train loss:0.11607421035175326\n",
            "train loss:0.15244818563382526\n",
            "train loss:0.21124456270383365\n",
            "train loss:0.267079516199125\n",
            "train loss:0.08835152005729999\n",
            "train loss:0.1796598916550942\n",
            "train loss:0.10005249253624146\n",
            "train loss:0.15346691356909517\n",
            "train loss:0.17863707766390374\n",
            "train loss:0.08991801440220307\n",
            "train loss:0.15884207989010105\n",
            "train loss:0.17977964302229296\n",
            "train loss:0.24290904375573086\n",
            "train loss:0.12774161842744164\n",
            "train loss:0.08206804122125023\n",
            "train loss:0.20720492802867058\n",
            "train loss:0.09067870063140057\n",
            "train loss:0.1702026834093341\n",
            "train loss:0.14345999745438728\n",
            "train loss:0.2053292936822568\n",
            "train loss:0.10433778239122062\n",
            "train loss:0.15095372581633565\n",
            "train loss:0.22015321877453642\n",
            "train loss:0.1206466534639203\n",
            "train loss:0.08957915688070814\n",
            "train loss:0.15209931901615845\n",
            "train loss:0.08861269416447348\n",
            "train loss:0.0954348128917758\n",
            "train loss:0.1442188457693975\n",
            "train loss:0.18278136123646643\n",
            "train loss:0.1491317552148632\n",
            "train loss:0.15993050481574217\n",
            "train loss:0.22503862722264192\n",
            "train loss:0.10757390840653322\n",
            "train loss:0.1474234263048067\n",
            "train loss:0.10780053563227252\n",
            "train loss:0.14844458837208366\n",
            "train loss:0.1285004448072807\n",
            "train loss:0.20824758266437868\n",
            "train loss:0.14613306396516992\n",
            "train loss:0.1793414330497771\n",
            "train loss:0.06946375558255204\n",
            "train loss:0.0912934729148845\n",
            "train loss:0.10432238693653917\n",
            "train loss:0.18012692857672102\n",
            "train loss:0.09867649076201802\n",
            "train loss:0.05305255065007097\n",
            "train loss:0.13059992056503\n",
            "train loss:0.09012983098427517\n",
            "train loss:0.08606594667756723\n",
            "train loss:0.16205419893094175\n",
            "train loss:0.10114402801422105\n",
            "train loss:0.12352032942373793\n",
            "train loss:0.07800878874717918\n",
            "train loss:0.08279398534933025\n",
            "train loss:0.0904443268719356\n",
            "train loss:0.04986921637843791\n",
            "train loss:0.0875624972786936\n",
            "train loss:0.1951690301985486\n",
            "train loss:0.05482839887352939\n",
            "train loss:0.16641172738002732\n",
            "train loss:0.09928372673670051\n",
            "train loss:0.07708305742287548\n",
            "train loss:0.10083791792135197\n",
            "train loss:0.07623844860247245\n",
            "train loss:0.10325791280436347\n",
            "train loss:0.12230144300426633\n",
            "train loss:0.12933690195357983\n",
            "train loss:0.08532308894209194\n",
            "train loss:0.185732264855453\n",
            "train loss:0.10569012384643256\n",
            "train loss:0.07310098586178207\n",
            "train loss:0.15721092043030901\n",
            "train loss:0.16018257718390522\n",
            "train loss:0.1002805965780313\n",
            "train loss:0.12266412129174382\n",
            "train loss:0.15354163288264863\n",
            "train loss:0.12027769367348201\n",
            "train loss:0.12222483785601934\n",
            "train loss:0.0585212544773945\n",
            "train loss:0.1886886379781496\n",
            "train loss:0.13651167032395053\n",
            "train loss:0.045569844231197816\n",
            "train loss:0.13717547837667005\n",
            "train loss:0.13857380913090772\n",
            "train loss:0.2250118034260088\n",
            "train loss:0.0648801511254606\n",
            "train loss:0.05734978608413152\n",
            "train loss:0.17864273072843975\n",
            "train loss:0.2859217124280324\n",
            "train loss:0.16239927981600638\n",
            "train loss:0.1155915143080667\n",
            "train loss:0.12075996466230517\n",
            "train loss:0.14666305523378864\n",
            "train loss:0.09262700783404912\n",
            "train loss:0.14949581139710122\n",
            "train loss:0.18963691124252338\n",
            "train loss:0.09969525413263208\n",
            "train loss:0.10233151919903721\n",
            "train loss:0.09576163883989208\n",
            "train loss:0.10500437038581198\n",
            "train loss:0.15370833584768803\n",
            "train loss:0.10070209520703888\n",
            "train loss:0.09026947238777709\n",
            "train loss:0.0899806418093911\n",
            "train loss:0.16237036442312824\n",
            "train loss:0.08866658022888413\n",
            "train loss:0.15789938247358495\n",
            "train loss:0.0950819336978781\n",
            "train loss:0.05848100569170407\n",
            "train loss:0.09013453695656158\n",
            "train loss:0.053153694125585974\n",
            "train loss:0.13906687447322721\n",
            "train loss:0.23853891257542614\n",
            "train loss:0.07762497906563463\n",
            "train loss:0.07331170584104775\n",
            "train loss:0.16973981679241873\n",
            "train loss:0.10226703632610967\n",
            "train loss:0.12696477235167347\n",
            "train loss:0.08892794381285185\n",
            "train loss:0.12833344510067268\n",
            "train loss:0.08476268211106275\n",
            "train loss:0.1010184599279367\n",
            "train loss:0.1290580165753314\n",
            "train loss:0.168546256838842\n",
            "train loss:0.07163719281286941\n",
            "train loss:0.07889918843304658\n",
            "train loss:0.14946932356538015\n",
            "train loss:0.1481474676088954\n",
            "train loss:0.10787513601324238\n",
            "train loss:0.19727057639854442\n",
            "train loss:0.1919473850147599\n",
            "train loss:0.06885027044105808\n",
            "train loss:0.09520881665138178\n",
            "train loss:0.1941934760894602\n",
            "train loss:0.05929587317608762\n",
            "train loss:0.11044556654272202\n",
            "train loss:0.0647751047441943\n",
            "train loss:0.15487021969172968\n",
            "train loss:0.051193025309821276\n",
            "train loss:0.09130982866527008\n",
            "train loss:0.1478130009597924\n",
            "train loss:0.1612961500043192\n",
            "train loss:0.07155705728143155\n",
            "train loss:0.12794480905418745\n",
            "train loss:0.03196704724865779\n",
            "train loss:0.1503772703781705\n",
            "train loss:0.10094380176609316\n",
            "train loss:0.08892681919771031\n",
            "train loss:0.07747166721826743\n",
            "train loss:0.06630860419363534\n",
            "train loss:0.33897609261623934\n",
            "train loss:0.06910373189598185\n",
            "train loss:0.08292893529029531\n",
            "train loss:0.15905178096349532\n",
            "train loss:0.06983886282004914\n",
            "train loss:0.14447646826456428\n",
            "train loss:0.08851589345837174\n",
            "train loss:0.06886590051410595\n",
            "train loss:0.15934705140217575\n",
            "train loss:0.09777629228895927\n",
            "train loss:0.209161048546201\n",
            "train loss:0.056852289467951046\n",
            "train loss:0.1303433267543199\n",
            "train loss:0.08223460257401244\n",
            "train loss:0.13247390039040574\n",
            "train loss:0.13494360626053348\n",
            "train loss:0.24360702891410072\n",
            "train loss:0.24436285386933732\n",
            "train loss:0.11004099384769324\n",
            "train loss:0.1399547864498151\n",
            "train loss:0.04524524485233547\n",
            "train loss:0.12739140042652675\n",
            "train loss:0.1532997026269947\n",
            "train loss:0.13308718885316415\n",
            "train loss:0.09377772224840897\n",
            "train loss:0.07973774430796204\n",
            "train loss:0.07890700811148879\n",
            "train loss:0.169177975481026\n",
            "train loss:0.1688603907613535\n",
            "train loss:0.06385499472717582\n",
            "train loss:0.0958694094319131\n",
            "train loss:0.13125381242285877\n",
            "train loss:0.1360977522029962\n",
            "train loss:0.11308943892729303\n",
            "train loss:0.04907700299898585\n",
            "train loss:0.1138604225400294\n",
            "train loss:0.0856464245363437\n",
            "train loss:0.10113010579945182\n",
            "train loss:0.17256262217343352\n",
            "train loss:0.08474191277952052\n",
            "train loss:0.16515185014155012\n",
            "train loss:0.11334042810573092\n",
            "train loss:0.11446711110714804\n",
            "train loss:0.03417522439200241\n",
            "train loss:0.08328494740466082\n",
            "train loss:0.09146353824866368\n",
            "train loss:0.026968970174277924\n",
            "train loss:0.1392875468051016\n",
            "train loss:0.02974120908302223\n",
            "train loss:0.050851266381207856\n",
            "train loss:0.07917869809343031\n",
            "train loss:0.04864089135803023\n",
            "train loss:0.12248966051469741\n",
            "train loss:0.03532595995097586\n",
            "train loss:0.13573777041793897\n",
            "train loss:0.16343274775771377\n",
            "train loss:0.041878147257054625\n",
            "train loss:0.14511499250976642\n",
            "train loss:0.13526214946412643\n",
            "train loss:0.09629859383043661\n",
            "train loss:0.10204857342266802\n",
            "train loss:0.09098636604458699\n",
            "train loss:0.1350349608901827\n",
            "train loss:0.09787641983725315\n",
            "train loss:0.08838915180942308\n",
            "train loss:0.08494099782089616\n",
            "train loss:0.04523035651785094\n",
            "train loss:0.10109976162512142\n",
            "train loss:0.10577818806290823\n",
            "train loss:0.038267167046517045\n",
            "train loss:0.054960573970205234\n",
            "train loss:0.11570068395771742\n",
            "train loss:0.07825717288614648\n",
            "train loss:0.06960823738212828\n",
            "train loss:0.044399058070200316\n",
            "train loss:0.06271969887164577\n",
            "train loss:0.1163866932756357\n",
            "train loss:0.04380299360769588\n",
            "train loss:0.07976514298518385\n",
            "train loss:0.05905276383948824\n",
            "train loss:0.0749760987593041\n",
            "train loss:0.08246829093942547\n",
            "train loss:0.049810288725290086\n",
            "train loss:0.13414448174346177\n",
            "train loss:0.20132691854780432\n",
            "train loss:0.14732604722957834\n",
            "train loss:0.12109378384948856\n",
            "train loss:0.08767540859815261\n",
            "train loss:0.09877026154803331\n",
            "train loss:0.11663145633936274\n",
            "train loss:0.05105927383299191\n",
            "train loss:0.07735739675740724\n",
            "train loss:0.10988440027429582\n",
            "train loss:0.07586434983509516\n",
            "train loss:0.12034828555508671\n",
            "train loss:0.10178694554475806\n",
            "train loss:0.04482803457150494\n",
            "train loss:0.08828104701674774\n",
            "train loss:0.298093317405316\n",
            "train loss:0.11753144528729993\n",
            "train loss:0.0691306163809905\n",
            "train loss:0.04566185043512957\n",
            "train loss:0.047364084178881105\n",
            "train loss:0.11564521739815303\n",
            "train loss:0.14245138248262357\n",
            "train loss:0.12288043459292008\n",
            "train loss:0.06682008318471117\n",
            "train loss:0.05746203771380643\n",
            "train loss:0.07508471396848865\n",
            "train loss:0.059281059772694655\n",
            "train loss:0.09228808614095861\n",
            "train loss:0.06555026786077917\n",
            "train loss:0.12212162751674853\n",
            "train loss:0.11733020514912453\n",
            "train loss:0.05466810341850455\n",
            "train loss:0.09229988988644278\n",
            "train loss:0.02385899461501014\n",
            "train loss:0.06812426117217472\n",
            "train loss:0.06580236104822965\n",
            "train loss:0.05802674969920676\n",
            "train loss:0.11072964764144061\n",
            "train loss:0.044161017710997515\n",
            "train loss:0.04948928175282787\n",
            "train loss:0.04641159085889275\n",
            "train loss:0.13015564258875228\n",
            "train loss:0.0454157139346297\n",
            "train loss:0.09821077361611867\n",
            "train loss:0.06664577009763421\n",
            "train loss:0.1542433030658034\n",
            "train loss:0.040541079625371255\n",
            "train loss:0.13272876615111565\n",
            "train loss:0.08756814563286419\n",
            "train loss:0.01503646013997762\n",
            "train loss:0.047068721339285836\n",
            "train loss:0.07108889282119281\n",
            "train loss:0.020046731292772138\n",
            "train loss:0.06536405071279518\n",
            "train loss:0.05106496376759192\n",
            "train loss:0.09395482413699088\n",
            "train loss:0.05077651651228086\n",
            "train loss:0.0824855909340589\n",
            "train loss:0.07497050154068742\n",
            "train loss:0.03842021429006254\n",
            "train loss:0.05452300665822631\n",
            "train loss:0.0959618387268554\n",
            "train loss:0.060063253520682286\n",
            "train loss:0.04716378397189278\n",
            "train loss:0.04888220151931982\n",
            "train loss:0.04376432534109491\n",
            "train loss:0.10694605841438895\n",
            "train loss:0.08334306971835367\n",
            "train loss:0.12547033306366742\n",
            "train loss:0.10381149941096245\n",
            "train loss:0.10550672163804074\n",
            "train loss:0.07186908963813837\n",
            "train loss:0.09547261798919962\n",
            "train loss:0.026714183597895153\n",
            "train loss:0.1189396755535672\n",
            "train loss:0.16756264514352698\n",
            "train loss:0.06347208728482442\n",
            "train loss:0.24962407851149582\n",
            "train loss:0.09070031129829367\n",
            "train loss:0.04121678698449648\n",
            "train loss:0.15146648045502015\n",
            "train loss:0.09561137288041126\n",
            "train loss:0.06726302895744883\n",
            "train loss:0.07955577975392408\n",
            "train loss:0.03976952998713719\n",
            "train loss:0.16548847367528022\n",
            "train loss:0.20422702811996593\n",
            "train loss:0.17111758596423443\n",
            "train loss:0.026922187034474118\n",
            "train loss:0.029865492551545744\n",
            "train loss:0.05714346276796535\n",
            "train loss:0.09068299590686639\n",
            "train loss:0.042342950510971644\n",
            "train loss:0.18029558471783655\n",
            "train loss:0.04286501087064971\n",
            "train loss:0.12391846907622123\n",
            "train loss:0.08008905648117842\n",
            "train loss:0.1243039657124869\n",
            "train loss:0.06285664533345338\n",
            "train loss:0.13948447517569235\n",
            "train loss:0.028600814230546293\n",
            "train loss:0.09738732805416336\n",
            "train loss:0.16435161046167882\n",
            "train loss:0.07384408834407981\n",
            "train loss:0.03468176549473278\n",
            "train loss:0.12177752182165817\n",
            "train loss:0.030918944377509007\n",
            "train loss:0.03417408150747305\n",
            "train loss:0.04316441523428371\n",
            "train loss:0.03654602597317748\n",
            "train loss:0.1131087589108835\n",
            "train loss:0.09022554777232684\n",
            "train loss:0.046144078368092464\n",
            "train loss:0.12763108674913373\n",
            "train loss:0.1058289547590454\n",
            "train loss:0.10253096554594718\n",
            "train loss:0.09239899833609351\n",
            "train loss:0.025496049422469608\n",
            "train loss:0.06682284139885658\n",
            "train loss:0.08499213926398085\n",
            "train loss:0.16052648711371886\n",
            "train loss:0.14129668564877412\n",
            "train loss:0.11357953444362481\n",
            "train loss:0.10667178106300966\n",
            "train loss:0.10699078986516258\n",
            "train loss:0.16225148900147596\n",
            "train loss:0.1012362654105611\n",
            "train loss:0.17435718194687344\n",
            "train loss:0.08218844969920637\n",
            "train loss:0.12396436452645516\n",
            "train loss:0.1330509618477269\n",
            "train loss:0.09694153837212287\n",
            "train loss:0.08928315297973674\n",
            "train loss:0.08467979987240326\n",
            "train loss:0.10838912318199537\n",
            "train loss:0.11907820866417217\n",
            "train loss:0.014774323940465572\n",
            "train loss:0.07671173213515288\n",
            "train loss:0.11703154853787069\n",
            "train loss:0.10852116620123636\n",
            "train loss:0.05234853770010651\n",
            "train loss:0.05692324878059945\n",
            "train loss:0.08375832216153102\n",
            "train loss:0.19181331824253398\n",
            "train loss:0.08593737255685324\n",
            "train loss:0.04326502516649103\n",
            "train loss:0.0409133398793566\n",
            "train loss:0.06269222644551053\n",
            "train loss:0.089531882642192\n",
            "train loss:0.06874708665343472\n",
            "train loss:0.12595301052884791\n",
            "train loss:0.10862847220532437\n",
            "train loss:0.09597776997181182\n",
            "train loss:0.08345938653539582\n",
            "train loss:0.16157219561370828\n",
            "train loss:0.13204240461726044\n",
            "train loss:0.09212674302625057\n",
            "train loss:0.09508595820528123\n",
            "train loss:0.0374600058131475\n",
            "train loss:0.1415852219032147\n",
            "train loss:0.05778841307000311\n",
            "train loss:0.05442548463210932\n",
            "train loss:0.18186765510321126\n",
            "train loss:0.02850652244333256\n",
            "train loss:0.1426659490739482\n",
            "train loss:0.057673605887319165\n",
            "train loss:0.04635317769004784\n",
            "train loss:0.05881002546108052\n",
            "train loss:0.04168758393473609\n",
            "train loss:0.06567586741262751\n",
            "train loss:0.10343923140591862\n",
            "train loss:0.04344668871673278\n",
            "train loss:0.09192132840336104\n",
            "train loss:0.10972355095747206\n",
            "train loss:0.05716181421539789\n",
            "train loss:0.03495427568368506\n",
            "train loss:0.03595134586331084\n",
            "train loss:0.03475769355586234\n",
            "train loss:0.08746172772744076\n",
            "train loss:0.0378336106606832\n",
            "train loss:0.07080244547845556\n",
            "train loss:0.07976306761796784\n",
            "train loss:0.1363987461739341\n",
            "train loss:0.09196679315744116\n",
            "train loss:0.05729096781922218\n",
            "train loss:0.06431420722932499\n",
            "train loss:0.020763413877458323\n",
            "train loss:0.12009756826356643\n",
            "train loss:0.08388686345010112\n",
            "train loss:0.11885467482849163\n",
            "train loss:0.08316033824256311\n",
            "train loss:0.1408132839212006\n",
            "train loss:0.14688126078310876\n",
            "train loss:0.16911914667074715\n",
            "train loss:0.06380756350821666\n",
            "train loss:0.04165594163153364\n",
            "train loss:0.2660545517797987\n",
            "train loss:0.04210492403556747\n",
            "train loss:0.10658444087684524\n",
            "train loss:0.13425916072282754\n",
            "train loss:0.04589994748496458\n",
            "train loss:0.06595510860313683\n",
            "train loss:0.05962265712233974\n",
            "train loss:0.26098284122502524\n",
            "train loss:0.11290965396548275\n",
            "train loss:0.05017256351019275\n",
            "train loss:0.082720571386078\n",
            "train loss:0.08281902018477078\n",
            "train loss:0.09905597781354152\n",
            "train loss:0.061203570919821676\n",
            "train loss:0.10671232870000619\n",
            "train loss:0.08348667001979862\n",
            "train loss:0.07500252190916468\n",
            "train loss:0.12445774758162342\n",
            "train loss:0.1074428957289092\n",
            "train loss:0.05252684672545487\n",
            "train loss:0.18233019345319662\n",
            "train loss:0.07121058319988763\n",
            "train loss:0.127852742042176\n",
            "train loss:0.1253931025577338\n",
            "train loss:0.1051622234287398\n",
            "train loss:0.1385672335759234\n",
            "train loss:0.06172569167802113\n",
            "train loss:0.09689325685399293\n",
            "train loss:0.16592892943427884\n",
            "train loss:0.05713540465360539\n",
            "train loss:0.20590499843196303\n",
            "train loss:0.1406437459550588\n",
            "train loss:0.12748246695025625\n",
            "train loss:0.08162299412825494\n",
            "train loss:0.06005007386099136\n",
            "train loss:0.11413818720897971\n",
            "train loss:0.07302369027864775\n",
            "train loss:0.04223492019779636\n",
            "train loss:0.04374195498819524\n",
            "train loss:0.0566854303610733\n",
            "train loss:0.07614436783005635\n",
            "train loss:0.057138590973224074\n",
            "train loss:0.05623039900727058\n",
            "train loss:0.07153260879471672\n",
            "train loss:0.18715935115048307\n",
            "train loss:0.045970497944203846\n",
            "train loss:0.0986314265826161\n",
            "train loss:0.04688255859518915\n",
            "train loss:0.08559145277440987\n",
            "train loss:0.04935925228198942\n",
            "train loss:0.11756609732144352\n",
            "train loss:0.0576920342688953\n",
            "train loss:0.13967459931205523\n",
            "train loss:0.08360084781460207\n",
            "train loss:0.03821937917787794\n",
            "train loss:0.09383797478820827\n",
            "train loss:0.04593076129995342\n",
            "train loss:0.05504916756753438\n",
            "train loss:0.15899764185491294\n",
            "train loss:0.08173755929049023\n",
            "train loss:0.037749673362913606\n",
            "train loss:0.039255847067514754\n",
            "train loss:0.07376673979318685\n",
            "train loss:0.04772863707720125\n",
            "train loss:0.031240776589634294\n",
            "train loss:0.13967508709891768\n",
            "train loss:0.17563360422907104\n",
            "train loss:0.07926958507039905\n",
            "train loss:0.1537301390328153\n",
            "train loss:0.08424060502447674\n",
            "train loss:0.201847378073914\n",
            "train loss:0.09235374930816631\n",
            "train loss:0.06355135346904164\n",
            "train loss:0.11708690158499836\n",
            "train loss:0.0882151336410206\n",
            "train loss:0.14243778854666997\n",
            "train loss:0.09010268462357876\n",
            "train loss:0.03281500914862665\n",
            "train loss:0.11562805707724323\n",
            "train loss:0.07973974989145631\n",
            "train loss:0.13884472737051928\n",
            "train loss:0.015014971844930091\n",
            "train loss:0.07866413384237365\n",
            "train loss:0.08451712597115532\n",
            "train loss:0.09987275222099568\n",
            "train loss:0.11537824182978376\n",
            "train loss:0.0968045930139831\n",
            "train loss:0.05933598331746368\n",
            "train loss:0.12113624108752447\n",
            "train loss:0.07276525507970734\n",
            "train loss:0.09055009985888311\n",
            "train loss:0.044325193823785004\n",
            "train loss:0.0840494920559143\n",
            "train loss:0.023431253867472256\n",
            "train loss:0.028966228480713004\n",
            "train loss:0.03851794944941461\n",
            "train loss:0.05820009460855819\n",
            "train loss:0.054978294094352564\n",
            "train loss:0.06075270171555037\n",
            "train loss:0.07959200238851491\n",
            "train loss:0.021938140006981747\n",
            "train loss:0.180580478119869\n",
            "train loss:0.04893204708865055\n",
            "train loss:0.08598422267797956\n",
            "train loss:0.12768936402816167\n",
            "train loss:0.11208720358583134\n",
            "train loss:0.057507201150618255\n",
            "train loss:0.08938285579986768\n",
            "train loss:0.036822728120078564\n",
            "train loss:0.06350970267413952\n",
            "train loss:0.0769187837141264\n",
            "train loss:0.07291727434744759\n",
            "train loss:0.027649127030291744\n",
            "train loss:0.04573926431080554\n",
            "train loss:0.08888050892233763\n",
            "train loss:0.06816415961676309\n",
            "train loss:0.10945269445399315\n",
            "train loss:0.058003678959488034\n",
            "train loss:0.058900880375763486\n",
            "train loss:0.08431531215607359\n",
            "train loss:0.03689193905729338\n",
            "train loss:0.10607563229277187\n",
            "train loss:0.06472084493517517\n",
            "train loss:0.11280778968801723\n",
            "train loss:0.07584581035400677\n",
            "train loss:0.07613529200868854\n",
            "train loss:0.0769269535837655\n",
            "train loss:0.03965797292608115\n",
            "train loss:0.057636737805241325\n",
            "train loss:0.08124525616397368\n",
            "train loss:0.10218057627738117\n",
            "train loss:0.11115764049971963\n",
            "train loss:0.07883989187908388\n",
            "train loss:0.02178791242913622\n",
            "train loss:0.05067375849198388\n",
            "train loss:0.02421793660078163\n",
            "train loss:0.039880805605579446\n",
            "train loss:0.04410264363318875\n",
            "train loss:0.045129892160430625\n",
            "train loss:0.04879017108803993\n",
            "train loss:0.056264575696657576\n",
            "train loss:0.058606076528151425\n",
            "train loss:0.02908203140553273\n",
            "train loss:0.07619393170704511\n",
            "train loss:0.1293498974596685\n",
            "train loss:0.04309191867669573\n",
            "train loss:0.10371856860509432\n",
            "train loss:0.12831771092835628\n",
            "train loss:0.08744768099886416\n",
            "=== epoch:3, train acc:0.972, test acc:0.976 ===\n",
            "train loss:0.11400390675117236\n",
            "train loss:0.03951531442512289\n",
            "train loss:0.019057076223902812\n",
            "train loss:0.03643858734095458\n",
            "train loss:0.10100025194807\n",
            "train loss:0.02918124082405683\n",
            "train loss:0.13203602929797056\n",
            "train loss:0.05736268808754558\n",
            "train loss:0.018709898722033935\n",
            "train loss:0.030588296327461295\n",
            "train loss:0.06857224957311042\n",
            "train loss:0.1095817858190263\n",
            "train loss:0.11986085995686213\n",
            "train loss:0.07671418424363516\n",
            "train loss:0.06377005289497609\n",
            "train loss:0.07895289665697063\n",
            "train loss:0.05930798719869564\n",
            "train loss:0.08413712807698576\n",
            "train loss:0.026731943486507183\n",
            "train loss:0.08957885597350863\n",
            "train loss:0.04855555209883728\n",
            "train loss:0.05258007008304027\n",
            "train loss:0.11419522160786466\n",
            "train loss:0.08574787237612837\n",
            "train loss:0.05361108548654185\n",
            "train loss:0.04894633877268691\n",
            "train loss:0.1396099228832419\n",
            "train loss:0.08534907908010507\n",
            "train loss:0.11679365461522412\n",
            "train loss:0.03882066495532852\n",
            "train loss:0.03045756273029246\n",
            "train loss:0.012480787970668207\n",
            "train loss:0.08952659863803104\n",
            "train loss:0.18251022212213935\n",
            "train loss:0.05132893067322894\n",
            "train loss:0.07972262693964885\n",
            "train loss:0.03884903636166951\n",
            "train loss:0.09497264191334302\n",
            "train loss:0.061457401838614346\n",
            "train loss:0.05378721978465108\n",
            "train loss:0.027731008551278125\n",
            "train loss:0.06243910594653003\n",
            "train loss:0.0715955226402526\n",
            "train loss:0.06326404697656825\n",
            "train loss:0.025276853309915207\n",
            "train loss:0.05726864376421749\n",
            "train loss:0.09775884191663824\n",
            "train loss:0.047640184797978734\n",
            "train loss:0.09553909229612245\n",
            "train loss:0.04243844968069635\n",
            "train loss:0.11764074517724873\n",
            "train loss:0.15986967039701597\n",
            "train loss:0.04075593943752437\n",
            "train loss:0.0485648773186161\n",
            "train loss:0.03389974039205003\n",
            "train loss:0.08795413767289265\n",
            "train loss:0.08008161823039948\n",
            "train loss:0.044605081891764\n",
            "train loss:0.09233980155016813\n",
            "train loss:0.10876697868716995\n",
            "train loss:0.0705919905708049\n",
            "train loss:0.06313841696094767\n",
            "train loss:0.06530123700591145\n",
            "train loss:0.12258930702426835\n",
            "train loss:0.05255755409757841\n",
            "train loss:0.03394985778674644\n",
            "train loss:0.041013315549099955\n",
            "train loss:0.13951506383907553\n",
            "train loss:0.04319215751204133\n",
            "train loss:0.02179496561898985\n",
            "train loss:0.037337452212418076\n",
            "train loss:0.06862054730117852\n",
            "train loss:0.0681690982592845\n",
            "train loss:0.02491207956769352\n",
            "train loss:0.036809584047982895\n",
            "train loss:0.06608775325098837\n",
            "train loss:0.07768043792627057\n",
            "train loss:0.07472836199158223\n",
            "train loss:0.021288886100458572\n",
            "train loss:0.12788553754031112\n",
            "train loss:0.04176961847048797\n",
            "train loss:0.10022014420540522\n",
            "train loss:0.13232650511098895\n",
            "train loss:0.04846111486538687\n",
            "train loss:0.1339303250397254\n",
            "train loss:0.07405762430268208\n",
            "train loss:0.017281120231675086\n",
            "train loss:0.0361318240764375\n",
            "train loss:0.11236002375880882\n",
            "train loss:0.08100280461599706\n",
            "train loss:0.06387177207490444\n",
            "train loss:0.04728488858499569\n",
            "train loss:0.02612201595974082\n",
            "train loss:0.08859111474382048\n",
            "train loss:0.033566148904712774\n",
            "train loss:0.07134961106554374\n",
            "train loss:0.04429916928848456\n",
            "train loss:0.04202167047681086\n",
            "train loss:0.06786501599781067\n",
            "train loss:0.06318689567310044\n",
            "train loss:0.07965487508089804\n",
            "train loss:0.07578274846300946\n",
            "train loss:0.03254105320204166\n",
            "train loss:0.09838799010674887\n",
            "train loss:0.032156310050967564\n",
            "train loss:0.023845807339222428\n",
            "train loss:0.06038773810862273\n",
            "train loss:0.05083995670282616\n",
            "train loss:0.08641281564270616\n",
            "train loss:0.04414261703651175\n",
            "train loss:0.06646446405490163\n",
            "train loss:0.05317964986953974\n",
            "train loss:0.07543887643111188\n",
            "train loss:0.05484631131620293\n",
            "train loss:0.042325793038097456\n",
            "train loss:0.061802337514186824\n",
            "train loss:0.11136983191333216\n",
            "train loss:0.033701049773531534\n",
            "train loss:0.025531725859657767\n",
            "train loss:0.12302843346139199\n",
            "train loss:0.05373183697291666\n",
            "train loss:0.14250201331446305\n",
            "train loss:0.06578448679583858\n",
            "train loss:0.12373015450600494\n",
            "train loss:0.10165016776580103\n",
            "train loss:0.023593225638522963\n",
            "train loss:0.0644023374831693\n",
            "train loss:0.08740111065875622\n",
            "train loss:0.09563065050738967\n",
            "train loss:0.03126524349326131\n",
            "train loss:0.023145226704340667\n",
            "train loss:0.05684936386387534\n",
            "train loss:0.0269757728445344\n",
            "train loss:0.04559422642252585\n",
            "train loss:0.12038746209505886\n",
            "train loss:0.044551055470787304\n",
            "train loss:0.047577479712664246\n",
            "train loss:0.04445049983920732\n",
            "train loss:0.018190143860663546\n",
            "train loss:0.025725385990551207\n",
            "train loss:0.053476131387183885\n",
            "train loss:0.037169466854748784\n",
            "train loss:0.0719590153330901\n",
            "train loss:0.028961567236128246\n",
            "train loss:0.14129610826519026\n",
            "train loss:0.031454022611622307\n",
            "train loss:0.061820333693705674\n",
            "train loss:0.11402044945192893\n",
            "train loss:0.11967025941758992\n",
            "train loss:0.09021468140047136\n",
            "train loss:0.08014204442319212\n",
            "train loss:0.10545972687671351\n",
            "train loss:0.12851925379735057\n",
            "train loss:0.05834359143608826\n",
            "train loss:0.14611563996000174\n",
            "train loss:0.054923436118079844\n",
            "train loss:0.07533270472485566\n",
            "train loss:0.05491609432889838\n",
            "train loss:0.04098215271057921\n",
            "train loss:0.09841449339885391\n",
            "train loss:0.05789280193421253\n",
            "train loss:0.04694756736689156\n",
            "train loss:0.024680725189950365\n",
            "train loss:0.22285336702363698\n",
            "train loss:0.04621897795146259\n",
            "train loss:0.180585833402221\n",
            "train loss:0.11369627522818564\n",
            "train loss:0.07083687849793316\n",
            "train loss:0.03859613676566577\n",
            "train loss:0.06428126291918079\n",
            "train loss:0.05750008450728794\n",
            "train loss:0.026657207502076036\n",
            "train loss:0.046569560489325694\n",
            "train loss:0.09506979448646671\n",
            "train loss:0.08623382921778572\n",
            "train loss:0.06606737456856616\n",
            "train loss:0.05905383215550172\n",
            "train loss:0.049241170082187494\n",
            "train loss:0.062012936048080086\n",
            "train loss:0.02316778381868877\n",
            "train loss:0.019526151889932963\n",
            "train loss:0.06481923623942851\n",
            "train loss:0.12184055600622662\n",
            "train loss:0.020264919167786655\n",
            "train loss:0.02803789826890319\n",
            "train loss:0.033606932112271594\n",
            "train loss:0.06657452543379595\n",
            "train loss:0.11452506036515075\n",
            "train loss:0.08698392668210246\n",
            "train loss:0.021342045386405703\n",
            "train loss:0.034786398010511664\n",
            "train loss:0.07700935798228345\n",
            "train loss:0.01246688317759726\n",
            "train loss:0.029886457746808186\n",
            "train loss:0.03693267846949404\n",
            "train loss:0.044651703069697526\n",
            "train loss:0.027593930793394058\n",
            "train loss:0.06381123540942804\n",
            "train loss:0.07600234082838365\n",
            "train loss:0.0647966883250752\n",
            "train loss:0.0682474627720459\n",
            "train loss:0.013145146002565696\n",
            "train loss:0.07711752205937485\n",
            "train loss:0.10926330537415177\n",
            "train loss:0.06686361812277716\n",
            "train loss:0.051104194043136264\n",
            "train loss:0.04322750715919955\n",
            "train loss:0.06875209280376553\n",
            "train loss:0.16212687274625776\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}