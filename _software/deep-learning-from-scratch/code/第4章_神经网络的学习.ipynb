{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "第4章 神经网络的学习.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bR24v8ivcJzl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR24v8ivcJzl"
      },
      "source": [
        "# 本节需要的函数\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntw1VEizWPEf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))    \n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "    \n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "    \n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 溢出对策\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val # 还原值\n",
        "        it.iternext()   \n",
        "        \n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR8dpA3xMmbN"
      },
      "source": [
        "# 神经网络的学习\n",
        "\n",
        "上一节也看到了，随机生成的参数要是能有高准确率那就有鬼了。当然，几千个参数靠人来设置也不现实。所以需要靠计算机来自动调整：\n",
        "\n",
        "* 对于一般的机器学习，机器会根据样本的“特征”来调整参数，而特征是由人来设置的。\n",
        "* 对于神经网络，我们直接将整个样本输入到网络，无需设置任何特征。\n",
        "\n",
        "因此神经网络的应用范围更广。\n",
        "\n",
        "## 训练数据与测试数据\n",
        "\n",
        "训练数据用于训练神经网络，寻找最优参数。\n",
        "\n",
        "测试数据用来测试神经网络。\n",
        "\n",
        "为什么要这样划分？因为如果只用训练数据来测试，那么很有可能神经网络只能识别训练数据，而无法识别其他数据。就好像做题，如果只做一类题，那就有可能在考试时做不出新的类型的题目了。这种做考试题的能力，在深度学习中叫 **泛化**；而只会做一类题叫 **过拟合**。\n",
        "\n",
        "## 损失函数\n",
        "\n",
        "我们用 **损失函数 loss function** 来衡量神经网络的误差。\n",
        "\n",
        "### 均方误差\n",
        "\n",
        "最简单的损失函数就是均方误差：\n",
        "\n",
        "$$\n",
        "E = \\frac{1}{2} \\sum_k (y_k - t_k)^2\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcTtSo1fTv3Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a73e1be8-e8c5-4b5c-ae18-f29f2d758374"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "y1 = np.array([0.1,0.7,0.2])\n",
        "y2 = np.array([0.1,0.5,0.4])\n",
        "t = np.array([0,1,0])\n",
        "\n",
        "[mean_squared_error(t,t),\n",
        " mean_squared_error(y1,t),\n",
        " mean_squared_error(y2,t)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 0.07000000000000002, 0.21000000000000002]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUhGubki_NHd"
      },
      "source": [
        "### 交叉熵误差\n",
        "\n",
        "$$\n",
        "E=-\\sum_k t_k \\log y_k\n",
        "$$\n",
        "\n",
        "输出的值只取决于正确标签对应的输出。如果输出是 $y=1$，则 $E=0$；如果输出 $y<1$，则 $E>0$。\n",
        "\n",
        "batch 学习的交叉熵误差：\n",
        "\n",
        "$$\n",
        "E=-\\frac{1}{N} \\sum_n \\sum_k t_{n,k} \\log y_{n,k}\n",
        "$$\n",
        "\n",
        "相当是所有训练数据的误差平均值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Er-TTxHocz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6feb88a8-72c2-4325-b736-c71c734c03c5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    \n",
        "    #加上微小值防止出现负无穷\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size \n",
        "\n",
        "y1 = np.array([0.1,0.7,0.2])\n",
        "y2 = np.array([0.1,0.5,0.4])\n",
        "t = np.array([0,1,0])\n",
        "\n",
        "[cross_entropy_error(t,t),\n",
        " cross_entropy_error(y1,t),\n",
        " cross_entropy_error(y2,t)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-9.999999505838704e-08, 0.3566748010815999, 0.6931469805599654]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbDdI3nw3dg3"
      },
      "source": [
        "## 能不能用准确率来作为损失函数\n",
        "\n",
        "显然是不能的。准确率指的是有多少个是正确的，比如100个数据中有32个正确，那精度就是32%。显然，准确率是不连续的，或者说是突变的，这样我们如果微调参数，那么准确率是不会变化的，这样就无法确定应该向哪个方向调整参数。这和阶跃函数无法作为激活函数的理由是一样的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzRZXX8oQOv0"
      },
      "source": [
        "## 梯度\n",
        "\n",
        "假如神经网络可以写成一个函数 $y=f(w, x)$ ，误差函数为 $L(f(w, x))$ ，为了使函数更准确，也就是误差函数最小。那么根据梯度的定义，我们可以向梯度方向减小参数：\n",
        "\n",
        "$$\n",
        "w_0' = w_0 - \\eta\\frac{\\partial L(f(x,w))}{\\partial w_0}\n",
        "$$\n",
        "\n",
        "$\\eta$ 称为**学习率**\n",
        "\n",
        "下面我们来看看如何求网络的梯度：\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNJxntk4WDxc"
      },
      "source": [
        "def numerical_gradient(f, x): #笨方法求梯度\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h) #取相近两点求斜率\n",
        "        \n",
        "        x[idx] = tmp_val # 还原值\n",
        "        it.iternext()   \n",
        "        \n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im56ZNH1VYlD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5e68da7c-f165-425d-897e-cec36d722dc2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class simpleNet:\n",
        "    def __init__(self):\n",
        "        self.W = np.random.randn(2,3) #随机生成参数\n",
        "\n",
        "    def predict(self, x): #预测\n",
        "        return np.dot(x, self.W)\n",
        "\n",
        "    def loss(self, x, t): #计算误差\n",
        "        z = self.predict(x)\n",
        "        y = softmax(z)\n",
        "        loss = cross_entropy_error(y, t)\n",
        "\n",
        "        return loss\n",
        "\n",
        "x = np.array([0.6, 0.9])\n",
        "t = np.array([0, 0, 1])\n",
        "\n",
        "net = simpleNet()\n",
        "\n",
        "f = lambda w: net.loss(x, t)\n",
        "dW = numerical_gradient(f, net.W)\n",
        "\n",
        "print(net.loss(x,t))\n",
        "print(dW)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2698596081175344\n",
            "[[ 0.11629091  0.02561713 -0.14190804]\n",
            " [ 0.17443636  0.03842569 -0.21286206]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nIGTJjRXu8M"
      },
      "source": [
        "求完梯度后更新梯度："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsvxCCA0YT_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a64d7b2d-a4ee-416b-e9b8-b4c9d420576f"
      },
      "source": [
        "eta = 1\n",
        "\n",
        "print(net.W)\n",
        "print(net.loss(x,t))\n",
        "\n",
        "for i in range(10): #利用梯度下降法更新十次参数\n",
        "    net.W = net.W - eta*numerical_gradient(f, net.W)\n",
        "\n",
        "print(net.W)\n",
        "print(net.loss(x,t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.52760346 -0.60826468  0.14784348]\n",
            " [-0.53011149 -2.15726367  0.54289599]]\n",
            "0.2698596081175344\n",
            "[[-0.98917483 -0.74641069  0.74756086]\n",
            " [-1.22246854 -2.36448269  1.44247207]]\n",
            "0.044317052837686284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGRdA9yAbu7Z"
      },
      "source": [
        "可以看出，训练 10 次后，误差已经减小了 2 个数量级。这就是我们希望达到的目标"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa99VcYMA8go"
      },
      "source": [
        "# 回到MNIST数据集\n",
        "\n",
        "我们尝试用一个两层的网络来识别 MNIST 数据集，我们将定义一个 TwoLayerNet，并实现相关的函数（预测、损失函数、精度、梯度……）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSSyKOWtBMyF"
      },
      "source": [
        "class TwoLayerNet: #一个简单的两层网路\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "        # 初始化权重\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #权重\n",
        "        self.params['b1'] = np.zeros(hidden_size) #偏置\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    def predict(self, x):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "    \n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        return y\n",
        "        \n",
        "    # x:输入数据, t:监督数据\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        \n",
        "        return cross_entropy_error(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x:输入数据, t:监督数据\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])  \n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "        grads = {}\n",
        "        \n",
        "        batch_num = x.shape[0]\n",
        "        \n",
        "        # forward\n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        # backward\n",
        "        dy = (y - t) / batch_num\n",
        "        grads['W2'] = np.dot(z1.T, dy)\n",
        "        grads['b2'] = np.sum(dy, axis=0)\n",
        "        \n",
        "        da1 = np.dot(dy, W2.T)\n",
        "        dz1 = sigmoid_grad(a1) * da1\n",
        "        grads['W1'] = np.dot(x.T, dz1)\n",
        "        grads['b1'] = np.sum(dz1, axis=0)\n",
        "\n",
        "        return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPg6HbGeJtmg"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACyfclImKfoY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "bc77baee-df9c-47fa-fbba-400cde49339c"
      },
      "source": [
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('You should use Python 3.x')\n",
        "import os.path\n",
        "from IPython.terminal.embed import InteractiveShellEmbed\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "## if you run in terminal, run this\n",
        "# dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "## if you run in IPython, run this\n",
        "ip_shell = InteractiveShellEmbed()\n",
        "dataset_dir = ip_shell.magic(\"%pwd\")\n",
        "\n",
        "save_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "train_num = 60000\n",
        "test_num = 10000\n",
        "img_dim = (1, 28, 28)\n",
        "img_size = 784\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print(\"Downloading \" + file_name + \" ... \")\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    print(\"Done\")\n",
        "    \n",
        "def download_mnist():\n",
        "    for v in key_file.values():\n",
        "       _download(v)\n",
        "        \n",
        "def _load_label(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    data = data.reshape(-1, img_size)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return data\n",
        "    \n",
        "def _convert_numpy():\n",
        "    dataset = {}\n",
        "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
        "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def init_mnist():\n",
        "    download_mnist()\n",
        "    dataset = _convert_numpy()\n",
        "    print(\"Creating pickle file ...\")\n",
        "    with open(save_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f, -1)\n",
        "    print(\"Done!\")\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "        \n",
        "    return T\n",
        "    \n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    \"\"\"读入MNIST数据集\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    normalize : 将图像的像素值正规化为0.0~1.0\n",
        "    one_hot_label : \n",
        "        one_hot_label为True的情况下，标签作为one-hot数组返回\n",
        "        one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组\n",
        "    flatten : 是否将图像展开为一维数组\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (训练图像, 训练标签), (测试图像, 测试标签)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(save_file):\n",
        "        init_mnist()\n",
        "        \n",
        "    with open(save_file, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    \n",
        "    if normalize:\n",
        "        for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].astype(np.float32)\n",
        "            dataset[key] /= 255.0\n",
        "            \n",
        "    if one_hot_label:\n",
        "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "    \n",
        "    if not flatten:\n",
        "         for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    init_mnist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW1xnvWUKH4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "c9d7960e-384f-4d18-ad89-1efe43f5fdbc"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 读入数据\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000  # 适当设定循环的次数\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 计算梯度\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 更新参数\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "# 绘制图形\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train acc, test acc | 0.09863333333333334, 0.0958\n",
            "train acc, test acc | 0.7995166666666667, 0.8065\n",
            "train acc, test acc | 0.8767, 0.8792\n",
            "train acc, test acc | 0.898, 0.9016\n",
            "train acc, test acc | 0.9086333333333333, 0.9114\n",
            "train acc, test acc | 0.9147666666666666, 0.9157\n",
            "train acc, test acc | 0.91985, 0.9204\n",
            "train acc, test acc | 0.9245, 0.9257\n",
            "train acc, test acc | 0.9285666666666667, 0.93\n",
            "train acc, test acc | 0.9315, 0.9325\n",
            "train acc, test acc | 0.93455, 0.9357\n",
            "train acc, test acc | 0.9379166666666666, 0.9383\n",
            "train acc, test acc | 0.94035, 0.9403\n",
            "train acc, test acc | 0.9420166666666666, 0.9408\n",
            "train acc, test acc | 0.9448666666666666, 0.9442\n",
            "train acc, test acc | 0.9472833333333334, 0.9468\n",
            "train acc, test acc | 0.9488166666666666, 0.947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}