{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "第5章 误差反向传播算法.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eXObXtPa6F7"
      },
      "source": [
        "# 本节需要用到的函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxrrQoCJazu7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def identity_function(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_function(x):\n",
        "    return np.array(x > 0, dtype=np.int)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))    \n",
        "\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "    \n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "    \n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x) # 溢出对策\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, t):\n",
        "    y = softmax(X)\n",
        "    return cross_entropy_error(y, t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nsYFgv5MPQz"
      },
      "source": [
        "def _numerical_gradient_1d(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val # 还原值\n",
        "        \n",
        "    return grad\n",
        "\n",
        "\n",
        "def numerical_gradient_2d(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return _numerical_gradient_1d(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "        \n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_1d(f, x)\n",
        "        \n",
        "        return grad\n",
        "\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x) # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val # 还原值\n",
        "        it.iternext()   \n",
        "        \n",
        "    return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zeun8mFTk7HB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "d262f0fb-c706-4940-a750-1c4191ef48f7"
      },
      "source": [
        "try:\n",
        "    import urllib.request\n",
        "except ImportError:\n",
        "    raise ImportError('You should use Python 3.x')\n",
        "import os.path\n",
        "from IPython.terminal.embed import InteractiveShellEmbed\n",
        "import gzip\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "## if you run in terminal, run this\n",
        "# dataset_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "## if you run in IPython, run this\n",
        "ip_shell = InteractiveShellEmbed()\n",
        "dataset_dir = ip_shell.magic(\"%pwd\")\n",
        "\n",
        "save_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "train_num = 60000\n",
        "test_num = 10000\n",
        "img_dim = (1, 28, 28)\n",
        "img_size = 784\n",
        "\n",
        "\n",
        "def _download(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    if os.path.exists(file_path):\n",
        "        return\n",
        "\n",
        "    print(\"Downloading \" + file_name + \" ... \")\n",
        "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
        "    print(\"Done\")\n",
        "    \n",
        "def download_mnist():\n",
        "    for v in key_file.values():\n",
        "       _download(v)\n",
        "        \n",
        "def _load_label(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    \n",
        "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    data = data.reshape(-1, img_size)\n",
        "    print(\"Done\")\n",
        "    \n",
        "    return data\n",
        "    \n",
        "def _convert_numpy():\n",
        "    dataset = {}\n",
        "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
        "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "def init_mnist():\n",
        "    download_mnist()\n",
        "    dataset = _convert_numpy()\n",
        "    print(\"Creating pickle file ...\")\n",
        "    with open(save_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f, -1)\n",
        "    print(\"Done!\")\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "        \n",
        "    return T\n",
        "    \n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    \"\"\"读入MNIST数据集\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    normalize : 将图像的像素值正规化为0.0~1.0\n",
        "    one_hot_label : \n",
        "        one_hot_label为True的情况下，标签作为one-hot数组返回\n",
        "        one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组\n",
        "    flatten : 是否将图像展开为一维数组\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (训练图像, 训练标签), (测试图像, 测试标签)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(save_file):\n",
        "        init_mnist()\n",
        "        \n",
        "    with open(save_file, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "    \n",
        "    if normalize:\n",
        "        for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].astype(np.float32)\n",
        "            dataset[key] /= 255.0\n",
        "            \n",
        "    if one_hot_label:\n",
        "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "    \n",
        "    if not flatten:\n",
        "         for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    init_mnist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz ... \n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz ... \n",
            "Done\n",
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7c02wsAFAWA"
      },
      "source": [
        "# 计算图\n",
        "\n",
        "尽管我们学过求导和链式求导法则，但我们依然要说说这个。因为很多深度学习框架就是靠这个来进行随机梯度下降（SGD）的。\n",
        "\n",
        "什么是计算图？假如我们有一个算式：$p=x+y$，我们可以用类似于神经元的方式画出这样的图：\n",
        "\n",
        "![](http://www.yiibai.com/uploads/images/201806/1406/738090622_96682.png)\n",
        "\n",
        "图中，每个节点是一个算符，算符对输入的数据进行计算，再输出值。通过多个算符的组合，可以表示出复杂的式子，比如：$(x+y)\\times z$\n",
        "\n",
        "![](http://www.yiibai.com/uploads/images/201806/1406/597090626_34819.png)\n",
        "\n",
        "计算图既可以将简单的计算组合起来，也可以将复杂的计算分解为简单的计算。\n",
        "\n",
        "从计算图的出发点到结束点的传播称为**正向传播**（forward propagation），从结束点向出发点传播，则为**反向传播**（backward propagation）。正向传播很好计算，但反向传播又如何计算呢？我们之前学过的求导的链式法则就派上用场了。\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*FceBJSJ7j8jHjb4TmLV0Ew.png)\n",
        "\n",
        "假如我们有一个节点是：$z=f(x,y)$，现在我们反向传播，已知了 $\\dfrac{\\partial L}{\\partial z}$，现在要求 $\\dfrac{\\partial L}{\\partial x}$，可以通过 $\\dfrac{\\partial L}{\\partial z}\\cdot\\dfrac{\\partial z}{\\partial x}$ 得到。以上面那个 $(x+y)\\times z$ 为例：\n",
        "\n",
        "![](http://www.yiibai.com/uploads/images/201806/1406/597090626_34819.png)\n",
        "\n",
        "1. 先考虑 g 到 p 的反向传播：$\\dfrac{\\partial g}{\\partial p}=z$\n",
        "2. 然后再考虑 p 到 x 的反向传播：$\\dfrac{\\partial p}{\\partial x}=1$\n",
        "3. 将两个组合起来：$\\dfrac{\\partial g}{\\partial p} \\rightarrow \\dfrac{\\partial p}{\\partial x}\\rightarrow z$\n",
        "\n",
        "可见，反向传播同正向传播一样，也是可以将简单的求导组合起来，得到输出到输入的求导。我们可以将中间变量储存起来，进行多次利用，从而节省求导时间。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozs4gVJuYfnw"
      },
      "source": [
        "# 简单层的实现\n",
        "\n",
        "## 乘法层和加法层\n",
        "\n",
        "乘法层：\n",
        "\n",
        "$$\n",
        "z=x\\cdot y\\\\\n",
        "\\frac{\\partial z}{\\partial x}=y\\\\\n",
        "\\frac{\\partial z}{\\partial y}=x\n",
        "$$\n",
        "\n",
        "加法层：\n",
        "\n",
        "$$\n",
        "z=x+ y\\\\\n",
        "\\frac{\\partial z}{\\partial x}=1\\\\\n",
        "\\frac{\\partial z}{\\partial y}=1\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHzTM13hYzFD"
      },
      "source": [
        "class MulLayer:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y                \n",
        "        out = x * y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * self.y\n",
        "        dy = dout * self.x\n",
        "\n",
        "        return dx, dy\n",
        "\n",
        "\n",
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * 1\n",
        "        dy = dout * 1\n",
        "\n",
        "        return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkde750zaG34"
      },
      "source": [
        "## ReLU层\n",
        "\n",
        "$$\n",
        "y=\n",
        "\\begin{cases}\n",
        "x, & (x>0)\\\\\n",
        "0, & (x\\leq 0)\n",
        "\\end{cases}\\\\\n",
        "\\frac{\\partial y}{\\partial x}=\\begin{cases}\n",
        "1, & (x>0)\\\\\n",
        "0, & (x\\leq 0)\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KOM7vtwayKc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0 #将小于0的x对应的输出置0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F6XaSi6bdgg"
      },
      "source": [
        "## Sigmoid层\n",
        "\n",
        "$$\n",
        "y=\\frac{1}{1+\\exp(-x)}\\\\\n",
        "\\frac{\\partial y}{\\partial x}=\\frac{\\exp(-x)}{[1+\\exp(-x)]^2}=y(1-y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekh44Orocfye"
      },
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiSYk-N2ce8z"
      },
      "source": [
        "## Affine层\n",
        "\n",
        "Affine 层包含多个权重以及偏置，其求导也相对复杂得多\n",
        "\n",
        "$$\n",
        "\\newcommand{\\bd}{\\boldsymbol}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\bd{X}\\cdot\\bd{W}+\\bd{B}=\\bd{Y}\n",
        "$$\n",
        "\n",
        "我们不妨假设：\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x_1 & x_2\n",
        "\\end{bmatrix} \\cdot\n",
        "\\begin{bmatrix}\n",
        "w_{11} & w_{12} & w_{13}\\\\\n",
        "w_{21} & w_{22} & w_{23}\n",
        "\\end{bmatrix} +\n",
        "\\begin{bmatrix}\n",
        "b_1 & b_2 & b_3\n",
        "\\end{bmatrix}=\n",
        "\\begin{bmatrix}\n",
        "y_1 & y_2 & y_3\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "根据[矩阵的求导法则](https://www.jianshu.com/p/6b64b7ee6ec2)，如果我们采用分子布局（求导时分子相同的放同一行）：\n",
        "\n",
        "$$\n",
        "\\dfrac{\\partial \\bd{Y}}{\\partial \\bd{X}} = \n",
        "\\begin{bmatrix}\n",
        "\\dfrac{\\partial y_1}{\\partial x_1} & \\dfrac{\\partial y_1}{\\partial x_2} \\\\ \n",
        "\\dfrac{\\partial y_2}{\\partial x_1} & \\dfrac{\\partial y_2}{\\partial x_2} \\\\ \n",
        "\\dfrac{\\partial y_3}{\\partial x_1} & \\dfrac{\\partial y_3}{\\partial x_2}\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "w_{11} & w_{21} \\\\ \n",
        "w_{12} & w_{22} \\\\ \n",
        "w_{13} & w_{23}\n",
        "\\end{bmatrix} = \\bd{W}^T\n",
        "$$\n",
        "\n",
        "同理：\n",
        "\n",
        "$$\n",
        "\\dfrac{\\partial \\bd{Y}}{\\partial \\bd{W}} = \\bd{X}^T \\quad\n",
        "\\dfrac{\\partial \\bd{Y}}{\\partial \\bd{B}} = \\bd{1}\n",
        "$$\n",
        "\n",
        "但要注意，如果考虑 $\\bd{Y}$ 反向传播之前的求导，即求：$\\dfrac{\\partial L}{\\partial \\bd{X}}$ 和 $\\dfrac{\\partial L}{\\partial \\bd{W}}$，则还要考虑矩阵乘法的顺序：\n",
        "\n",
        "$$\n",
        "\\dfrac{\\partial L}{\\partial \\bd{X}} = \\dfrac{\\partial L}{\\partial \\bd{Y}} \\cdot \\bd{W}^T\\\\\n",
        "\\dfrac{\\partial L}{\\partial \\bd{W}} = \\bd{X}^T \\cdot \\dfrac{\\partial L}{\\partial \\bd{Y}}\n",
        "$$\n",
        "\n",
        "从行列数匹配的角度去想会好理解一点。因为 $\\dfrac{\\partial L}{\\partial \\bd{X}} = \\begin{bmatrix}\\dfrac{\\partial L}{\\partial x_1} & \\dfrac{\\partial L}{\\partial x_2} \\end{bmatrix}$，$\\dfrac{\\partial L}{\\partial \\bd{W}} = \\begin{bmatrix}\\dfrac{\\partial L}{\\partial w_1} & \\dfrac{\\partial L}{\\partial w_2} & \\dfrac{\\partial L}{\\partial w_3}\\end{bmatrix}$，而 $\\dfrac{\\partial \\bd{Y}}{\\partial \\bd{X}}= \\bd{W}^T$ 是一个 $3\\times2$ 的矩阵，所以必须是 $\\dfrac{\\partial L}{\\partial \\bd{X}} = \\dfrac{\\partial L}{\\partial \\bd{Y}} \\cdot \\bd{W}^T$。另一个也是一样的道理，但写起来稍微复杂一点，在这里就不写了。\n",
        "\n",
        "如果考虑 $\\bd{X}$ 是二维、三维、四维的情况，那么我们对偏置求偏导时，需要将 $\\dfrac{\\partial L}{\\partial \\bd{Y}}$ 0维对应的数据相加，即：$\\dfrac{\\partial L}{\\partial \\bd{B}} = \\begin{bmatrix}\\sum\\dfrac{\\partial L}{\\partial y_{i1}} & \\sum \\dfrac{\\partial L}{\\partial x_{i2}} & \\cdots \\end{bmatrix}$。这是因为对于多维的情况，偏置$\\bd{B}$ 会先进行扩充，然后再相加到各个数据上，所以反过来，求导的时候则需要合并。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxDeQOYStPVE"
      },
      "source": [
        "下面是批处理版本的代码。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9qURuqcc8YB"
      },
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W #权重\n",
        "        self.b = b #偏置\n",
        "        \n",
        "        self.x = None #输入\n",
        "        self.original_x_shape = None #原输入的形状\n",
        "        # 权重和偏置参数的导数\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 对应张量\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T) #对应上面的公式\n",
        "        self.dW = np.dot(self.x.T, dout) #对应上面的公式\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）\n",
        "        return dx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJk8ElOQ63kP"
      },
      "source": [
        "# Softmax 层\n",
        "\n",
        "$$\n",
        "y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^n \\exp(a_i)}\n",
        "$$\n",
        "\n",
        "一般我们会结合 loss 层一起考虑。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE7A9_2--eje"
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None # softmax的输出\n",
        "        self.t = None # 监督数据\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TCBND4ALL9C"
      },
      "source": [
        "# 误差反向传播算法的实现\n",
        "\n",
        "与之前一样，我们同样是生成一个两层的神经网络，唯一不同的是，神经网络增加了一个误差反向传播求梯度的函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIL47zFPLW5t"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 初始化权重\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 生成层\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "        \n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    # x:输入数据, t:监督数据\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x:输入数据, t:监督数据\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 设定\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSnpWYoWLu6Z"
      },
      "source": [
        "训练网络："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj07pNxnLt_A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "f9db6375-9cbb-42e9-f98c-7d562bde933c"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 梯度\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 更新\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.12738333333333332 0.1288\n",
            "0.90455 0.909\n",
            "0.92495 0.928\n",
            "0.9365 0.9395\n",
            "0.9444166666666667 0.946\n",
            "0.9498 0.9504\n",
            "0.9550333333333333 0.9544\n",
            "0.9592666666666667 0.9566\n",
            "0.95915 0.9562\n",
            "0.9624333333333334 0.9585\n",
            "0.9674333333333334 0.9643\n",
            "0.9693166666666667 0.9628\n",
            "0.9727166666666667 0.9672\n",
            "0.9727 0.9658\n",
            "0.9735166666666667 0.9673\n",
            "0.975 0.968\n",
            "0.9780833333333333 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfy6PQIVMytC"
      },
      "source": [
        "我们可以将反向传播与之前的数值微分得到的值进行比较："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p_6uAl-Mm7o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c3d326ce-9350-4636-8879-144e584f45f7"
      },
      "source": [
        "# 读入数据\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "    print(key + \":\" + str(diff))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1:4.2968520032354943e-10\n",
            "b1:2.6641099171273617e-09\n",
            "W2:4.796070617839067e-09\n",
            "b2:1.3916415531195492e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyZ_ZFBM68m"
      },
      "source": [
        "可以发现，两个结果十分相近，说明我们反向传播算法是正确的。"
      ]
    }
  ]
}